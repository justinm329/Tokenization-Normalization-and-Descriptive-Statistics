{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you pulled lyrics data on two artists. In this assignment we explore this data set and a pull from the now-defunct Twitter API for the artists Cher and Robyn.  If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Canvas. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa737920",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/justinfarnan_hakko\n",
      "[nltk_data]     da/ads_text_mining/M2/Tokenization-Normalization-and-\n",
      "[nltk_data]     Descriptive-Statistics/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/justinfarnan_hakkoda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "nltk.download('stopwords', download_dir='/Users/justinfarnan_hakkoda/ads_text_mining/M2/Tokenization-Normalization-and-Descriptive-Statistics/nltk_data')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "sw = stopwords.words(\"english\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/justinfarnan_hakkoda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/justinfarnan_hakkoda/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add any additional import statements you need here\n",
    "import csv\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "# Code to download corpora\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/justinfarnan_hakkoda/ads_text_mining/M2/Tokenization-Normalization-and-Descriptive-Statistics/M1_Results\n"
     ]
    }
   ],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/Users/justinfarnan_hakkoda/ads_text_mining/M2/Tokenization-Normalization-and-Descriptive-Statistics/M1_Results\"\n",
    "print(data_location)\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = len(set(tokens)) / len(tokens)\n",
    "    num_characters = (sum(len(token) for token in tokens))\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n"
     ]
    }
   ],
   "source": [
    "text = \"here is some example text with other example text here in this text\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: Assertion statements are a great way to go about testing and debugging your code. It is also beneficial cause you are able to do error handling with assertion statements. Basically what they do is go through and see if the statment inside or condition inside is true and if it is not it throws an Assetion Error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37d70801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lyrics data\n",
    "\n",
    "lyrics_data = pd.DataFrame(columns = ['artist', 'song', 'lyris'])\n",
    "\n",
    "for artist in os.listdir(f'{data_location}/lyrics'):\n",
    "    artist_folder = os.path.join(data_location, 'lyrics', artist)\n",
    "    for song in os.listdir(artist_folder):\n",
    "        song_file = os.path.join(artist_folder, song)\n",
    "        with open(song_file, 'r') as file:\n",
    "            lyrics = file.read()\n",
    "            new_row = pd.DataFrame({'artist': [artist], 'song': [song], 'lyrics': [lyrics]})\n",
    "            lyrics_data = pd.concat([lyrics_data, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb378f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyris</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robyn</td>\n",
       "      <td>robyn_includemeout.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Include Me Out\"\\n\\n\\n\\nIt is really very simp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robyn</td>\n",
       "      <td>robyn_electric.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Electric\"\\n\\n\\n\\nElectric...\\n\\nIt's electric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robyn</td>\n",
       "      <td>robyn_beach2k20.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Beach 2K20\"\\n\\n\\n\\n(So you wanna go out?\\nHow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robyn</td>\n",
       "      <td>robyn_lovekills.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Love Kills\"\\n\\n\\n\\nIf you're looking for love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robyn</td>\n",
       "      <td>robyn_timemachine.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Time Machine\"\\n\\n\\n\\nHey, what did I do?\\nCan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_takeitfromtheboys.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Take It From The Boys\"\\n\\n\\n\\nSo scared I nev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_dreambaby.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Dream Baby\"\\n\\n\\n\\nI found the boy for me\\nHe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_pleasedonttellme.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Please Don't Tell Me\"\\n\\n\\n\\nYa shook the ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_ihopeyoufindit.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"I Hope You Find It\"\\n\\n\\n\\nThese clouds aren'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_classified1a.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Classified 1A\"\\n\\n\\n\\nI know now how much I l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist                        song lyris  \\\n",
       "0    robyn      robyn_includemeout.txt   NaN   \n",
       "1    robyn          robyn_electric.txt   NaN   \n",
       "2    robyn         robyn_beach2k20.txt   NaN   \n",
       "3    robyn         robyn_lovekills.txt   NaN   \n",
       "4    robyn       robyn_timemachine.txt   NaN   \n",
       "..     ...                         ...   ...   \n",
       "415   cher  cher_takeitfromtheboys.txt   NaN   \n",
       "416   cher          cher_dreambaby.txt   NaN   \n",
       "417   cher   cher_pleasedonttellme.txt   NaN   \n",
       "418   cher     cher_ihopeyoufindit.txt   NaN   \n",
       "419   cher       cher_classified1a.txt   NaN   \n",
       "\n",
       "                                                lyrics  \n",
       "0    \"Include Me Out\"\\n\\n\\n\\nIt is really very simp...  \n",
       "1    \"Electric\"\\n\\n\\n\\nElectric...\\n\\nIt's electric...  \n",
       "2    \"Beach 2K20\"\\n\\n\\n\\n(So you wanna go out?\\nHow...  \n",
       "3    \"Love Kills\"\\n\\n\\n\\nIf you're looking for love...  \n",
       "4    \"Time Machine\"\\n\\n\\n\\nHey, what did I do?\\nCan...  \n",
       "..                                                 ...  \n",
       "415  \"Take It From The Boys\"\\n\\n\\n\\nSo scared I nev...  \n",
       "416  \"Dream Baby\"\\n\\n\\n\\nI found the boy for me\\nHe...  \n",
       "417  \"Please Don't Tell Me\"\\n\\n\\n\\nYa shook the ove...  \n",
       "418  \"I Hope You Find It\"\\n\\n\\n\\nThese clouds aren'...  \n",
       "419  \"Classified 1A\"\\n\\n\\n\\nI know now how much I l...  \n",
       "\n",
       "[420 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab4c0c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/justinfarnan_hakkoda/ads_text_mining/M2/Tokenization-Normalization-and-Descriptive-Statistics/M1_Results/twitter/cher_followers_data.txt\n",
      "/Users/justinfarnan_hakkoda/ads_text_mining/M2/Tokenization-Normalization-and-Descriptive-Statistics/M1_Results/twitter/robynkonichiwa_followers_data.txt\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "twitter_dfs = []\n",
    "for file in os.listdir(f'{data_location}/twitter'):\n",
    "    if file == \".DS_Store\":\n",
    "        continue\n",
    "    if file.endswith(\"_data.txt\"):  # Check for \"_data.txt\" specifically\n",
    "        file_path = os.path.join(data_location, 'twitter', file)\n",
    "        print(file_path)\n",
    "        with open(file_path, 'r') as file_obj:\n",
    "            content = file_obj.read()\n",
    "            lines = content.splitlines()\n",
    "            column_names = lines[0].split('\\t')\n",
    "            data = [line.split('\\t') for line in lines[1:]]\n",
    "            twitter_df = pd.DataFrame(data, columns=column_names)\n",
    "            twitter_dfs.append(twitter_df)\n",
    "print(len(twitter_dfs))  # Check the length of the list\n",
    "twitter_df = pd.concat(twitter_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2676e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/justinfarnan_hakkoda/ads_text_mining/M2/Tokenization-Normalization-and-Descriptive-Statistics/M1_Results/twitter/cher_followers.txt\n",
      "/Users/justinfarnan_hakkoda/ads_text_mining/M2/Tokenization-Normalization-and-Descriptive-Statistics/M1_Results/twitter/robynkonichiwa_followers.txt\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "follower_ids_dfs = []\n",
    "for file in os.listdir(f'{data_location}/twitter'):\n",
    "    if file == \".DS_Store\":\n",
    "        continue\n",
    "    if file.endswith(\"followers.txt\"):  # Check for \"followers.txt\" specifically\n",
    "        file_path = os.path.join(data_location, 'twitter', file)\n",
    "        print(file_path)\n",
    "        artist_name = file.split('_')[0]  # Extract the artist name from the file name\n",
    "        with open(file_path, 'r') as file_obj:\n",
    "            content = file_obj.read()\n",
    "            lines = content.splitlines()\n",
    "            follower_ids = [line.split('\\t')[0] for line in lines[1:]]  # Extract the first column (follower IDs)\n",
    "            follower_ids_df = pd.DataFrame(follower_ids, columns=['id'])\n",
    "            follower_ids_df['artist_name'] = artist_name  # Add the artist name as a new column\n",
    "            follower_ids_dfs.append(follower_ids_df)\n",
    "print(len(follower_ids_dfs))  # Check the length of the list\n",
    "follower_ids_concat = pd.concat(follower_ids_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcfdbe6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hsmcnp</td>\n",
       "      <td>Country Girl</td>\n",
       "      <td>35152213</td>\n",
       "      <td></td>\n",
       "      <td>1302</td>\n",
       "      <td>1014</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>horrormomy</td>\n",
       "      <td>Jeny</td>\n",
       "      <td>742153090850164742</td>\n",
       "      <td>Earth</td>\n",
       "      <td>81</td>\n",
       "      <td>514</td>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 &amp; 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anju79990584</td>\n",
       "      <td>anju</td>\n",
       "      <td>1496463006451974150</td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>140</td>\n",
       "      <td>163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gallionjenna</td>\n",
       "      <td>J</td>\n",
       "      <td>3366479914</td>\n",
       "      <td></td>\n",
       "      <td>752</td>\n",
       "      <td>556</td>\n",
       "      <td>csu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bcscomm</td>\n",
       "      <td>bcscomm</td>\n",
       "      <td>83915043</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>888</td>\n",
       "      <td>2891</td>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    screen_name          name                   id        location  \\\n",
       "0        hsmcnp  Country Girl             35152213                   \n",
       "1    horrormomy          Jeny   742153090850164742           Earth   \n",
       "2  anju79990584          anju  1496463006451974150                   \n",
       "3  gallionjenna             J           3366479914                   \n",
       "4       bcscomm       bcscomm             83915043  Washington, DC   \n",
       "\n",
       "  followers_count friends_count  \\\n",
       "0            1302          1014   \n",
       "1              81           514   \n",
       "2              13           140   \n",
       "3             752           556   \n",
       "4             888          2891   \n",
       "\n",
       "                                         description  \n",
       "0                                                     \n",
       "1           𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜  \n",
       "2          163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡  \n",
       "3                                                csu  \n",
       "4  Writer @Washinformer @SpelmanCollege alumna #D...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb44e8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>artist_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>358456</th>\n",
       "      <td>517669236</td>\n",
       "      <td>robynkonichiwa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358457</th>\n",
       "      <td>333029341</td>\n",
       "      <td>robynkonichiwa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358458</th>\n",
       "      <td>397990675</td>\n",
       "      <td>robynkonichiwa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358459</th>\n",
       "      <td>60245011</td>\n",
       "      <td>robynkonichiwa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358460</th>\n",
       "      <td>44946331</td>\n",
       "      <td>robynkonichiwa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id     artist_name\n",
       "358456  517669236  robynkonichiwa\n",
       "358457  333029341  robynkonichiwa\n",
       "358458  397990675  robynkonichiwa\n",
       "358459   60245011  robynkonichiwa\n",
       "358460   44946331  robynkonichiwa"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "follower_ids_concat.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "835ce34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_twitter_data = pd.merge(twitter_df, follower_ids_concat, on = 'id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29c95fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>description</th>\n",
       "      <th>artist_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hsmcnp</td>\n",
       "      <td>Country Girl</td>\n",
       "      <td>35152213</td>\n",
       "      <td></td>\n",
       "      <td>1302</td>\n",
       "      <td>1014</td>\n",
       "      <td></td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>horrormomy</td>\n",
       "      <td>Jeny</td>\n",
       "      <td>742153090850164742</td>\n",
       "      <td>Earth</td>\n",
       "      <td>81</td>\n",
       "      <td>514</td>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 &amp; 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anju79990584</td>\n",
       "      <td>anju</td>\n",
       "      <td>1496463006451974150</td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>140</td>\n",
       "      <td>163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gallionjenna</td>\n",
       "      <td>J</td>\n",
       "      <td>3366479914</td>\n",
       "      <td></td>\n",
       "      <td>752</td>\n",
       "      <td>556</td>\n",
       "      <td>csu</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bcscomm</td>\n",
       "      <td>bcscomm</td>\n",
       "      <td>83915043</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>888</td>\n",
       "      <td>2891</td>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    screen_name          name                   id        location  \\\n",
       "0        hsmcnp  Country Girl             35152213                   \n",
       "1    horrormomy          Jeny   742153090850164742           Earth   \n",
       "2  anju79990584          anju  1496463006451974150                   \n",
       "3  gallionjenna             J           3366479914                   \n",
       "4       bcscomm       bcscomm             83915043  Washington, DC   \n",
       "\n",
       "  followers_count friends_count  \\\n",
       "0            1302          1014   \n",
       "1              81           514   \n",
       "2              13           140   \n",
       "3             752           556   \n",
       "4             888          2891   \n",
       "\n",
       "                                         description artist_name  \n",
       "0                                                           cher  \n",
       "1           𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜        cher  \n",
       "2          163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡        cher  \n",
       "3                                                csu        cher  \n",
       "4  Writer @Washinformer @SpelmanCollege alumna #D...        cher  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_twitter_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01b5e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "def clean_tokenize(text):\n",
    "    # Remove punctuation\n",
    "    text_no_punct = ''.join(char for char in text if char not in punctuation)\n",
    "    # Split on whitespace\n",
    "    tokens = word_tokenize(text_no_punct)\n",
    "    # Fold to lowercase\n",
    "    tokens_lower = [token.lower() for token in tokens]\n",
    "    # Remove stopwords\n",
    "    tokens_no_stopwords = [token for token in tokens_lower if token not in stopwords_list]\n",
    "    return tokens_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b327033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your clean twitter data here\n",
    "joined_twitter_data['cleaned_description'] = joined_twitter_data['description'].apply(clean_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1a108c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>description</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>cleaned_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hsmcnp</td>\n",
       "      <td>Country Girl</td>\n",
       "      <td>35152213</td>\n",
       "      <td></td>\n",
       "      <td>1302</td>\n",
       "      <td>1014</td>\n",
       "      <td></td>\n",
       "      <td>cher</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>horrormomy</td>\n",
       "      <td>Jeny</td>\n",
       "      <td>742153090850164742</td>\n",
       "      <td>Earth</td>\n",
       "      <td>81</td>\n",
       "      <td>514</td>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 &amp; 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "      <td>cher</td>\n",
       "      <td>[𝙿𝚛𝚘𝚞𝚍, 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛, 𝚘𝚏, 𝚖𝚎𝚜𝚜𝚢, 𝚋𝚞𝚗𝚜, 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anju79990584</td>\n",
       "      <td>anju</td>\n",
       "      <td>1496463006451974150</td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>140</td>\n",
       "      <td>163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "      <td>cher</td>\n",
       "      <td>[163㎝／愛かっぷ💜26歳🍒, 工〇好きな女の子💓, フォローしてくれたらdmします🧡]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gallionjenna</td>\n",
       "      <td>J</td>\n",
       "      <td>3366479914</td>\n",
       "      <td></td>\n",
       "      <td>752</td>\n",
       "      <td>556</td>\n",
       "      <td>csu</td>\n",
       "      <td>cher</td>\n",
       "      <td>[csu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bcscomm</td>\n",
       "      <td>bcscomm</td>\n",
       "      <td>83915043</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>888</td>\n",
       "      <td>2891</td>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "      <td>cher</td>\n",
       "      <td>[writer, washinformer, spelmancollege, alumna,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    screen_name          name                   id        location  \\\n",
       "0        hsmcnp  Country Girl             35152213                   \n",
       "1    horrormomy          Jeny   742153090850164742           Earth   \n",
       "2  anju79990584          anju  1496463006451974150                   \n",
       "3  gallionjenna             J           3366479914                   \n",
       "4       bcscomm       bcscomm             83915043  Washington, DC   \n",
       "\n",
       "  followers_count friends_count  \\\n",
       "0            1302          1014   \n",
       "1              81           514   \n",
       "2              13           140   \n",
       "3             752           556   \n",
       "4             888          2891   \n",
       "\n",
       "                                         description artist_name  \\\n",
       "0                                                           cher   \n",
       "1           𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜        cher   \n",
       "2          163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡        cher   \n",
       "3                                                csu        cher   \n",
       "4  Writer @Washinformer @SpelmanCollege alumna #D...        cher   \n",
       "\n",
       "                                 cleaned_description  \n",
       "0                                                 []  \n",
       "1      [𝙿𝚛𝚘𝚞𝚍, 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛, 𝚘𝚏, 𝚖𝚎𝚜𝚜𝚢, 𝚋𝚞𝚗𝚜, 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜]  \n",
       "2      [163㎝／愛かっぷ💜26歳🍒, 工〇好きな女の子💓, フォローしてくれたらdmします🧡]  \n",
       "3                                              [csu]  \n",
       "4  [writer, washinformer, spelmancollege, alumna,...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see if the tokeninzation and cleaning went properly\n",
    "joined_twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0f22e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your clean lyrics data here\n",
    "lyrics_data['lyrics_clean'] = lyrics_data['lyrics'].apply(clean_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6f539ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyris</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robyn</td>\n",
       "      <td>robyn_includemeout.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Include Me Out\"\\n\\n\\n\\nIt is really very simp...</td>\n",
       "      <td>[include, really, simple, single, pulse, repea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robyn</td>\n",
       "      <td>robyn_electric.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Electric\"\\n\\n\\n\\nElectric...\\n\\nIt's electric...</td>\n",
       "      <td>[electric, electric, electric, natural, high, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robyn</td>\n",
       "      <td>robyn_beach2k20.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Beach 2K20\"\\n\\n\\n\\n(So you wanna go out?\\nHow...</td>\n",
       "      <td>[beach, 2k20, wan, na, go, gon, na, get, ok, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robyn</td>\n",
       "      <td>robyn_lovekills.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Love Kills\"\\n\\n\\n\\nIf you're looking for love...</td>\n",
       "      <td>[love, kills, youre, looking, love, get, heart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robyn</td>\n",
       "      <td>robyn_timemachine.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Time Machine\"\\n\\n\\n\\nHey, what did I do?\\nCan...</td>\n",
       "      <td>[time, machine, hey, cant, believe, fit, threw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_takeitfromtheboys.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Take It From The Boys\"\\n\\n\\n\\nSo scared I nev...</td>\n",
       "      <td>[take, boys, scared, never, hard, keep, good, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_dreambaby.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Dream Baby\"\\n\\n\\n\\nI found the boy for me\\nHe...</td>\n",
       "      <td>[dream, baby, found, boy, hes, dream, baby, do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_pleasedonttellme.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Please Don't Tell Me\"\\n\\n\\n\\nYa shook the ove...</td>\n",
       "      <td>[please, dont, tell, ya, shook, override, whyd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_ihopeyoufindit.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"I Hope You Find It\"\\n\\n\\n\\nThese clouds aren'...</td>\n",
       "      <td>[hope, find, clouds, arent, going, nowhere, da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_classified1a.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Classified 1A\"\\n\\n\\n\\nI know now how much I l...</td>\n",
       "      <td>[classified, 1a, know, much, love, knew, surel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist                        song lyris  \\\n",
       "0    robyn      robyn_includemeout.txt   NaN   \n",
       "1    robyn          robyn_electric.txt   NaN   \n",
       "2    robyn         robyn_beach2k20.txt   NaN   \n",
       "3    robyn         robyn_lovekills.txt   NaN   \n",
       "4    robyn       robyn_timemachine.txt   NaN   \n",
       "..     ...                         ...   ...   \n",
       "415   cher  cher_takeitfromtheboys.txt   NaN   \n",
       "416   cher          cher_dreambaby.txt   NaN   \n",
       "417   cher   cher_pleasedonttellme.txt   NaN   \n",
       "418   cher     cher_ihopeyoufindit.txt   NaN   \n",
       "419   cher       cher_classified1a.txt   NaN   \n",
       "\n",
       "                                                lyrics  \\\n",
       "0    \"Include Me Out\"\\n\\n\\n\\nIt is really very simp...   \n",
       "1    \"Electric\"\\n\\n\\n\\nElectric...\\n\\nIt's electric...   \n",
       "2    \"Beach 2K20\"\\n\\n\\n\\n(So you wanna go out?\\nHow...   \n",
       "3    \"Love Kills\"\\n\\n\\n\\nIf you're looking for love...   \n",
       "4    \"Time Machine\"\\n\\n\\n\\nHey, what did I do?\\nCan...   \n",
       "..                                                 ...   \n",
       "415  \"Take It From The Boys\"\\n\\n\\n\\nSo scared I nev...   \n",
       "416  \"Dream Baby\"\\n\\n\\n\\nI found the boy for me\\nHe...   \n",
       "417  \"Please Don't Tell Me\"\\n\\n\\n\\nYa shook the ove...   \n",
       "418  \"I Hope You Find It\"\\n\\n\\n\\nThese clouds aren'...   \n",
       "419  \"Classified 1A\"\\n\\n\\n\\nI know now how much I l...   \n",
       "\n",
       "                                          lyrics_clean  \n",
       "0    [include, really, simple, single, pulse, repea...  \n",
       "1    [electric, electric, electric, natural, high, ...  \n",
       "2    [beach, 2k20, wan, na, go, gon, na, get, ok, c...  \n",
       "3    [love, kills, youre, looking, love, get, heart...  \n",
       "4    [time, machine, hey, cant, believe, fit, threw...  \n",
       "..                                                 ...  \n",
       "415  [take, boys, scared, never, hard, keep, good, ...  \n",
       "416  [dream, baby, found, boy, hes, dream, baby, do...  \n",
       "417  [please, dont, tell, ya, shook, override, whyd...  \n",
       "418  [hope, find, clouds, arent, going, nowhere, da...  \n",
       "419  [classified, 1a, know, much, love, knew, surel...  \n",
       "\n",
       "[420 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 235 tokens in the data.\n",
      "There are 89 unique tokens in the data.\n",
      "There are 1186 characters in the data.\n",
      "The lexical diversity is 0.379 in the data.\n",
      "There are 153 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 818 characters in the data.\n",
      "The lexical diversity is 0.412 in the data.\n",
      "There are 184 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 713 characters in the data.\n",
      "The lexical diversity is 0.190 in the data.\n",
      "There are 246 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 1162 characters in the data.\n",
      "The lexical diversity is 0.171 in the data.\n",
      "There are 129 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 624 characters in the data.\n",
      "The lexical diversity is 0.411 in the data.\n",
      "There are 247 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 1167 characters in the data.\n",
      "The lexical diversity is 0.170 in the data.\n",
      "There are 113 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 530 characters in the data.\n",
      "The lexical diversity is 0.619 in the data.\n",
      "There are 126 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 555 characters in the data.\n",
      "The lexical diversity is 0.468 in the data.\n",
      "There are 126 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 555 characters in the data.\n",
      "The lexical diversity is 0.468 in the data.\n",
      "There are 175 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 857 characters in the data.\n",
      "The lexical diversity is 0.474 in the data.\n",
      "There are 249 tokens in the data.\n",
      "There are 114 unique tokens in the data.\n",
      "There are 1352 characters in the data.\n",
      "The lexical diversity is 0.458 in the data.\n",
      "There are 98 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 480 characters in the data.\n",
      "The lexical diversity is 0.684 in the data.\n",
      "There are 108 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 521 characters in the data.\n",
      "The lexical diversity is 0.769 in the data.\n",
      "There are 103 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 550 characters in the data.\n",
      "The lexical diversity is 0.621 in the data.\n",
      "There are 141 tokens in the data.\n",
      "There are 96 unique tokens in the data.\n",
      "There are 701 characters in the data.\n",
      "The lexical diversity is 0.681 in the data.\n",
      "There are 152 tokens in the data.\n",
      "There are 87 unique tokens in the data.\n",
      "There are 731 characters in the data.\n",
      "The lexical diversity is 0.572 in the data.\n",
      "There are 283 tokens in the data.\n",
      "There are 89 unique tokens in the data.\n",
      "There are 1638 characters in the data.\n",
      "The lexical diversity is 0.314 in the data.\n",
      "There are 100 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 510 characters in the data.\n",
      "The lexical diversity is 0.680 in the data.\n",
      "There are 113 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 479 characters in the data.\n",
      "The lexical diversity is 0.389 in the data.\n",
      "There are 90 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 417 characters in the data.\n",
      "The lexical diversity is 0.689 in the data.\n",
      "There are 95 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 455 characters in the data.\n",
      "The lexical diversity is 0.684 in the data.\n",
      "There are 315 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 1440 characters in the data.\n",
      "The lexical diversity is 0.152 in the data.\n",
      "There are 164 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 731 characters in the data.\n",
      "The lexical diversity is 0.488 in the data.\n",
      "There are 192 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 887 characters in the data.\n",
      "The lexical diversity is 0.318 in the data.\n",
      "There are 146 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 713 characters in the data.\n",
      "The lexical diversity is 0.295 in the data.\n",
      "There are 98 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 480 characters in the data.\n",
      "The lexical diversity is 0.684 in the data.\n",
      "There are 77 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 319 characters in the data.\n",
      "The lexical diversity is 0.545 in the data.\n",
      "There are 226 tokens in the data.\n",
      "There are 81 unique tokens in the data.\n",
      "There are 999 characters in the data.\n",
      "The lexical diversity is 0.358 in the data.\n",
      "There are 83 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 435 characters in the data.\n",
      "The lexical diversity is 0.711 in the data.\n",
      "There are 142 tokens in the data.\n",
      "There are 97 unique tokens in the data.\n",
      "There are 690 characters in the data.\n",
      "The lexical diversity is 0.683 in the data.\n",
      "There are 52 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 222 characters in the data.\n",
      "The lexical diversity is 0.269 in the data.\n",
      "There are 131 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 640 characters in the data.\n",
      "The lexical diversity is 0.366 in the data.\n",
      "There are 74 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 338 characters in the data.\n",
      "The lexical diversity is 0.378 in the data.\n",
      "There are 170 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 762 characters in the data.\n",
      "The lexical diversity is 0.318 in the data.\n",
      "There are 70 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 305 characters in the data.\n",
      "The lexical diversity is 0.600 in the data.\n",
      "There are 139 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 614 characters in the data.\n",
      "The lexical diversity is 0.424 in the data.\n",
      "There are 148 tokens in the data.\n",
      "There are 69 unique tokens in the data.\n",
      "There are 659 characters in the data.\n",
      "The lexical diversity is 0.466 in the data.\n",
      "There are 158 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 735 characters in the data.\n",
      "The lexical diversity is 0.468 in the data.\n",
      "There are 187 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 879 characters in the data.\n",
      "The lexical diversity is 0.332 in the data.\n",
      "There are 249 tokens in the data.\n",
      "There are 114 unique tokens in the data.\n",
      "There are 1352 characters in the data.\n",
      "The lexical diversity is 0.458 in the data.\n",
      "There are 222 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 993 characters in the data.\n",
      "The lexical diversity is 0.275 in the data.\n",
      "There are 98 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 531 characters in the data.\n",
      "The lexical diversity is 0.643 in the data.\n",
      "There are 148 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 670 characters in the data.\n",
      "The lexical diversity is 0.453 in the data.\n",
      "There are 139 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 603 characters in the data.\n",
      "The lexical diversity is 0.417 in the data.\n",
      "There are 182 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 815 characters in the data.\n",
      "The lexical diversity is 0.319 in the data.\n",
      "There are 277 tokens in the data.\n",
      "There are 132 unique tokens in the data.\n",
      "There are 1384 characters in the data.\n",
      "The lexical diversity is 0.477 in the data.\n",
      "There are 147 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 554 characters in the data.\n",
      "The lexical diversity is 0.422 in the data.\n",
      "There are 200 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 954 characters in the data.\n",
      "The lexical diversity is 0.370 in the data.\n",
      "There are 109 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 478 characters in the data.\n",
      "The lexical diversity is 0.495 in the data.\n",
      "There are 166 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 785 characters in the data.\n",
      "The lexical diversity is 0.307 in the data.\n",
      "There are 222 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 1129 characters in the data.\n",
      "The lexical diversity is 0.324 in the data.\n",
      "There are 187 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 879 characters in the data.\n",
      "The lexical diversity is 0.332 in the data.\n",
      "There are 74 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 297 characters in the data.\n",
      "The lexical diversity is 0.486 in the data.\n",
      "There are 177 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 1078 characters in the data.\n",
      "The lexical diversity is 0.226 in the data.\n",
      "There are 66 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 279 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "There are 124 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 530 characters in the data.\n",
      "The lexical diversity is 0.290 in the data.\n",
      "There are 120 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 578 characters in the data.\n",
      "The lexical diversity is 0.475 in the data.\n",
      "There are 55 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 266 characters in the data.\n",
      "The lexical diversity is 0.764 in the data.\n",
      "There are 87 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 400 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "There are 351 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 1542 characters in the data.\n",
      "The lexical diversity is 0.177 in the data.\n",
      "There are 101 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 467 characters in the data.\n",
      "The lexical diversity is 0.752 in the data.\n",
      "There are 150 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 748 characters in the data.\n",
      "The lexical diversity is 0.493 in the data.\n",
      "There are 84 tokens in the data.\n",
      "There are 30 unique tokens in the data.\n",
      "There are 375 characters in the data.\n",
      "The lexical diversity is 0.357 in the data.\n",
      "There are 106 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 537 characters in the data.\n",
      "The lexical diversity is 0.311 in the data.\n",
      "There are 77 tokens in the data.\n",
      "There are 34 unique tokens in the data.\n",
      "There are 343 characters in the data.\n",
      "The lexical diversity is 0.442 in the data.\n",
      "There are 325 tokens in the data.\n",
      "There are 184 unique tokens in the data.\n",
      "There are 1600 characters in the data.\n",
      "The lexical diversity is 0.566 in the data.\n",
      "There are 170 tokens in the data.\n",
      "There are 162 unique tokens in the data.\n",
      "There are 1042 characters in the data.\n",
      "The lexical diversity is 0.953 in the data.\n",
      "There are 82 tokens in the data.\n",
      "There are 30 unique tokens in the data.\n",
      "There are 350 characters in the data.\n",
      "The lexical diversity is 0.366 in the data.\n",
      "There are 325 tokens in the data.\n",
      "There are 184 unique tokens in the data.\n",
      "There are 1600 characters in the data.\n",
      "The lexical diversity is 0.566 in the data.\n",
      "There are 131 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 639 characters in the data.\n",
      "The lexical diversity is 0.458 in the data.\n",
      "There are 169 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 823 characters in the data.\n",
      "The lexical diversity is 0.373 in the data.\n",
      "There are 138 tokens in the data.\n",
      "There are 25 unique tokens in the data.\n",
      "There are 510 characters in the data.\n",
      "The lexical diversity is 0.181 in the data.\n",
      "There are 181 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 812 characters in the data.\n",
      "The lexical diversity is 0.320 in the data.\n",
      "There are 194 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 894 characters in the data.\n",
      "The lexical diversity is 0.299 in the data.\n",
      "There are 102 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 524 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "There are 177 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 1078 characters in the data.\n",
      "The lexical diversity is 0.226 in the data.\n",
      "There are 146 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 713 characters in the data.\n",
      "The lexical diversity is 0.295 in the data.\n",
      "There are 85 tokens in the data.\n",
      "There are 32 unique tokens in the data.\n",
      "There are 450 characters in the data.\n",
      "The lexical diversity is 0.376 in the data.\n",
      "There are 188 tokens in the data.\n",
      "There are 90 unique tokens in the data.\n",
      "There are 829 characters in the data.\n",
      "The lexical diversity is 0.479 in the data.\n",
      "There are 205 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 887 characters in the data.\n",
      "The lexical diversity is 0.390 in the data.\n",
      "There are 82 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 366 characters in the data.\n",
      "The lexical diversity is 0.683 in the data.\n",
      "There are 91 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 436 characters in the data.\n",
      "The lexical diversity is 0.582 in the data.\n",
      "There are 257 tokens in the data.\n",
      "There are 96 unique tokens in the data.\n",
      "There are 1240 characters in the data.\n",
      "The lexical diversity is 0.374 in the data.\n",
      "There are 104 tokens in the data.\n",
      "There are 38 unique tokens in the data.\n",
      "There are 483 characters in the data.\n",
      "The lexical diversity is 0.365 in the data.\n",
      "There are 110 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 547 characters in the data.\n",
      "The lexical diversity is 0.409 in the data.\n",
      "There are 182 tokens in the data.\n",
      "There are 77 unique tokens in the data.\n",
      "There are 856 characters in the data.\n",
      "The lexical diversity is 0.423 in the data.\n",
      "There are 199 tokens in the data.\n",
      "There are 152 unique tokens in the data.\n",
      "There are 994 characters in the data.\n",
      "The lexical diversity is 0.764 in the data.\n",
      "There are 97 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 447 characters in the data.\n",
      "The lexical diversity is 0.588 in the data.\n",
      "There are 148 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 670 characters in the data.\n",
      "The lexical diversity is 0.453 in the data.\n",
      "There are 216 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 961 characters in the data.\n",
      "The lexical diversity is 0.236 in the data.\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 67 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 29 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 123 characters in the data.\n",
      "The lexical diversity is 0.552 in the data.\n",
      "There are 48 tokens in the data.\n",
      "There are 32 unique tokens in the data.\n",
      "There are 246 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "There are 52 tokens in the data.\n",
      "There are 39 unique tokens in the data.\n",
      "There are 257 characters in the data.\n",
      "The lexical diversity is 0.750 in the data.\n",
      "There are 228 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 1028 characters in the data.\n",
      "The lexical diversity is 0.268 in the data.\n",
      "There are 152 tokens in the data.\n",
      "There are 84 unique tokens in the data.\n",
      "There are 729 characters in the data.\n",
      "The lexical diversity is 0.553 in the data.\n",
      "There are 96 tokens in the data.\n",
      "There are 29 unique tokens in the data.\n",
      "There are 459 characters in the data.\n",
      "The lexical diversity is 0.302 in the data.\n",
      "There are 95 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 486 characters in the data.\n",
      "The lexical diversity is 0.747 in the data.\n",
      "There are 222 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 1129 characters in the data.\n",
      "The lexical diversity is 0.324 in the data.\n",
      "There are 114 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 520 characters in the data.\n",
      "The lexical diversity is 0.614 in the data.\n",
      "There are 173 tokens in the data.\n",
      "There are 97 unique tokens in the data.\n",
      "There are 779 characters in the data.\n",
      "The lexical diversity is 0.561 in the data.\n",
      "There are 188 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 1003 characters in the data.\n",
      "The lexical diversity is 0.404 in the data.\n",
      "There are 67 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 342 characters in the data.\n",
      "The lexical diversity is 0.701 in the data.\n",
      "There are 96 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 448 characters in the data.\n",
      "The lexical diversity is 0.573 in the data.\n",
      "There are 80 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 342 characters in the data.\n",
      "The lexical diversity is 0.600 in the data.\n",
      "There are 147 tokens in the data.\n",
      "There are 79 unique tokens in the data.\n",
      "There are 637 characters in the data.\n",
      "The lexical diversity is 0.537 in the data.\n",
      "There are 160 tokens in the data.\n",
      "There are 106 unique tokens in the data.\n",
      "There are 774 characters in the data.\n",
      "The lexical diversity is 0.662 in the data.\n",
      "There are 82 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 386 characters in the data.\n",
      "The lexical diversity is 0.659 in the data.\n",
      "There are 89 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 408 characters in the data.\n",
      "The lexical diversity is 0.663 in the data.\n",
      "There are 78 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 389 characters in the data.\n",
      "The lexical diversity is 0.577 in the data.\n",
      "There are 84 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 358 characters in the data.\n",
      "The lexical diversity is 0.560 in the data.\n",
      "There are 125 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 586 characters in the data.\n",
      "The lexical diversity is 0.528 in the data.\n",
      "There are 73 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 378 characters in the data.\n",
      "The lexical diversity is 0.479 in the data.\n",
      "There are 199 tokens in the data.\n",
      "There are 118 unique tokens in the data.\n",
      "There are 964 characters in the data.\n",
      "The lexical diversity is 0.593 in the data.\n",
      "There are 196 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 802 characters in the data.\n",
      "The lexical diversity is 0.301 in the data.\n",
      "There are 152 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 686 characters in the data.\n",
      "The lexical diversity is 0.395 in the data.\n",
      "There are 101 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 447 characters in the data.\n",
      "The lexical diversity is 0.465 in the data.\n",
      "There are 110 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 491 characters in the data.\n",
      "The lexical diversity is 0.664 in the data.\n",
      "There are 124 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 555 characters in the data.\n",
      "The lexical diversity is 0.573 in the data.\n",
      "There are 83 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 373 characters in the data.\n",
      "The lexical diversity is 0.337 in the data.\n",
      "There are 69 tokens in the data.\n",
      "There are 49 unique tokens in the data.\n",
      "There are 299 characters in the data.\n",
      "The lexical diversity is 0.710 in the data.\n",
      "There are 134 tokens in the data.\n",
      "There are 81 unique tokens in the data.\n",
      "There are 622 characters in the data.\n",
      "The lexical diversity is 0.604 in the data.\n",
      "There are 243 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 1128 characters in the data.\n",
      "The lexical diversity is 0.288 in the data.\n",
      "There are 124 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 547 characters in the data.\n",
      "The lexical diversity is 0.282 in the data.\n",
      "There are 183 tokens in the data.\n",
      "There are 115 unique tokens in the data.\n",
      "There are 965 characters in the data.\n",
      "The lexical diversity is 0.628 in the data.\n",
      "There are 131 tokens in the data.\n",
      "There are 101 unique tokens in the data.\n",
      "There are 655 characters in the data.\n",
      "The lexical diversity is 0.771 in the data.\n",
      "There are 52 tokens in the data.\n",
      "There are 38 unique tokens in the data.\n",
      "There are 275 characters in the data.\n",
      "The lexical diversity is 0.731 in the data.\n",
      "There are 74 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 332 characters in the data.\n",
      "The lexical diversity is 0.473 in the data.\n",
      "There are 155 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 783 characters in the data.\n",
      "The lexical diversity is 0.342 in the data.\n",
      "There are 110 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 490 characters in the data.\n",
      "The lexical diversity is 0.509 in the data.\n",
      "There are 126 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 564 characters in the data.\n",
      "The lexical diversity is 0.659 in the data.\n",
      "There are 88 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 429 characters in the data.\n",
      "The lexical diversity is 0.841 in the data.\n",
      "There are 123 tokens in the data.\n",
      "There are 93 unique tokens in the data.\n",
      "There are 667 characters in the data.\n",
      "The lexical diversity is 0.756 in the data.\n",
      "There are 108 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 467 characters in the data.\n",
      "The lexical diversity is 0.370 in the data.\n",
      "There are 72 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 330 characters in the data.\n",
      "The lexical diversity is 0.569 in the data.\n",
      "There are 67 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 320 characters in the data.\n",
      "The lexical diversity is 0.493 in the data.\n",
      "There are 56 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 256 characters in the data.\n",
      "The lexical diversity is 0.732 in the data.\n",
      "There are 112 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 458 characters in the data.\n",
      "The lexical diversity is 0.464 in the data.\n",
      "There are 80 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 335 characters in the data.\n",
      "The lexical diversity is 0.525 in the data.\n",
      "There are 106 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 577 characters in the data.\n",
      "The lexical diversity is 0.585 in the data.\n",
      "There are 111 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 534 characters in the data.\n",
      "The lexical diversity is 0.532 in the data.\n",
      "There are 70 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 324 characters in the data.\n",
      "The lexical diversity is 0.857 in the data.\n",
      "There are 94 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 467 characters in the data.\n",
      "The lexical diversity is 0.755 in the data.\n",
      "There are 104 tokens in the data.\n",
      "There are 50 unique tokens in the data.\n",
      "There are 548 characters in the data.\n",
      "The lexical diversity is 0.481 in the data.\n",
      "There are 144 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 703 characters in the data.\n",
      "The lexical diversity is 0.354 in the data.\n",
      "There are 175 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 752 characters in the data.\n",
      "The lexical diversity is 0.411 in the data.\n",
      "There are 113 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 526 characters in the data.\n",
      "The lexical diversity is 0.389 in the data.\n",
      "There are 105 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 451 characters in the data.\n",
      "The lexical diversity is 0.343 in the data.\n",
      "There are 57 tokens in the data.\n",
      "There are 23 unique tokens in the data.\n",
      "There are 301 characters in the data.\n",
      "The lexical diversity is 0.404 in the data.\n",
      "There are 205 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 888 characters in the data.\n",
      "The lexical diversity is 0.317 in the data.\n",
      "There are 88 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 421 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "There are 114 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 501 characters in the data.\n",
      "The lexical diversity is 0.509 in the data.\n",
      "There are 45 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 200 characters in the data.\n",
      "The lexical diversity is 0.622 in the data.\n",
      "There are 143 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 814 characters in the data.\n",
      "The lexical diversity is 0.573 in the data.\n",
      "There are 145 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 713 characters in the data.\n",
      "The lexical diversity is 0.303 in the data.\n",
      "There are 145 tokens in the data.\n",
      "There are 99 unique tokens in the data.\n",
      "There are 751 characters in the data.\n",
      "The lexical diversity is 0.683 in the data.\n",
      "There are 45 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 168 characters in the data.\n",
      "The lexical diversity is 0.400 in the data.\n",
      "There are 153 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 699 characters in the data.\n",
      "The lexical diversity is 0.536 in the data.\n",
      "There are 182 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 831 characters in the data.\n",
      "The lexical diversity is 0.451 in the data.\n",
      "There are 112 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 581 characters in the data.\n",
      "The lexical diversity is 0.571 in the data.\n",
      "There are 124 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 676 characters in the data.\n",
      "The lexical diversity is 0.597 in the data.\n",
      "There are 70 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 319 characters in the data.\n",
      "The lexical diversity is 0.771 in the data.\n",
      "There are 93 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 430 characters in the data.\n",
      "The lexical diversity is 0.376 in the data.\n",
      "There are 144 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 626 characters in the data.\n",
      "The lexical diversity is 0.424 in the data.\n",
      "There are 110 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 433 characters in the data.\n",
      "The lexical diversity is 0.300 in the data.\n",
      "There are 87 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 398 characters in the data.\n",
      "The lexical diversity is 0.460 in the data.\n",
      "There are 125 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 557 characters in the data.\n",
      "The lexical diversity is 0.496 in the data.\n",
      "There are 76 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 363 characters in the data.\n",
      "The lexical diversity is 0.368 in the data.\n",
      "There are 174 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 893 characters in the data.\n",
      "The lexical diversity is 0.379 in the data.\n",
      "There are 73 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 390 characters in the data.\n",
      "The lexical diversity is 0.452 in the data.\n",
      "There are 88 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 395 characters in the data.\n",
      "The lexical diversity is 0.455 in the data.\n",
      "There are 120 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 650 characters in the data.\n",
      "The lexical diversity is 0.525 in the data.\n",
      "There are 110 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 507 characters in the data.\n",
      "The lexical diversity is 0.400 in the data.\n",
      "There are 60 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 318 characters in the data.\n",
      "The lexical diversity is 0.583 in the data.\n",
      "There are 80 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 352 characters in the data.\n",
      "The lexical diversity is 0.675 in the data.\n",
      "There are 145 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 744 characters in the data.\n",
      "The lexical diversity is 0.572 in the data.\n",
      "There are 152 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 662 characters in the data.\n",
      "The lexical diversity is 0.336 in the data.\n",
      "There are 47 tokens in the data.\n",
      "There are 25 unique tokens in the data.\n",
      "There are 188 characters in the data.\n",
      "The lexical diversity is 0.532 in the data.\n",
      "There are 67 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 339 characters in the data.\n",
      "The lexical diversity is 0.687 in the data.\n",
      "There are 176 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 941 characters in the data.\n",
      "The lexical diversity is 0.432 in the data.\n",
      "There are 218 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 1108 characters in the data.\n",
      "The lexical diversity is 0.271 in the data.\n",
      "There are 140 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 605 characters in the data.\n",
      "The lexical diversity is 0.571 in the data.\n",
      "There are 210 tokens in the data.\n",
      "There are 78 unique tokens in the data.\n",
      "There are 883 characters in the data.\n",
      "The lexical diversity is 0.371 in the data.\n",
      "There are 133 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 497 characters in the data.\n",
      "The lexical diversity is 0.414 in the data.\n",
      "There are 95 tokens in the data.\n",
      "There are 78 unique tokens in the data.\n",
      "There are 483 characters in the data.\n",
      "The lexical diversity is 0.821 in the data.\n",
      "There are 142 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 655 characters in the data.\n",
      "The lexical diversity is 0.387 in the data.\n",
      "There are 183 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 822 characters in the data.\n",
      "The lexical diversity is 0.344 in the data.\n",
      "There are 87 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 450 characters in the data.\n",
      "The lexical diversity is 0.759 in the data.\n",
      "There are 142 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 663 characters in the data.\n",
      "The lexical diversity is 0.310 in the data.\n",
      "There are 80 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 364 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "There are 94 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 497 characters in the data.\n",
      "The lexical diversity is 0.617 in the data.\n",
      "There are 133 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 618 characters in the data.\n",
      "The lexical diversity is 0.504 in the data.\n",
      "There are 102 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 482 characters in the data.\n",
      "The lexical diversity is 0.431 in the data.\n",
      "There are 145 tokens in the data.\n",
      "There are 86 unique tokens in the data.\n",
      "There are 699 characters in the data.\n",
      "The lexical diversity is 0.593 in the data.\n",
      "There are 84 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 376 characters in the data.\n",
      "The lexical diversity is 0.393 in the data.\n",
      "There are 114 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 514 characters in the data.\n",
      "The lexical diversity is 0.561 in the data.\n",
      "There are 144 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 692 characters in the data.\n",
      "The lexical diversity is 0.451 in the data.\n",
      "There are 73 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 389 characters in the data.\n",
      "The lexical diversity is 0.630 in the data.\n",
      "There are 99 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 499 characters in the data.\n",
      "The lexical diversity is 0.687 in the data.\n",
      "There are 108 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 512 characters in the data.\n",
      "The lexical diversity is 0.583 in the data.\n",
      "There are 73 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 391 characters in the data.\n",
      "The lexical diversity is 0.808 in the data.\n",
      "There are 175 tokens in the data.\n",
      "There are 85 unique tokens in the data.\n",
      "There are 917 characters in the data.\n",
      "The lexical diversity is 0.486 in the data.\n",
      "There are 117 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 595 characters in the data.\n",
      "The lexical diversity is 0.462 in the data.\n",
      "There are 159 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 722 characters in the data.\n",
      "The lexical diversity is 0.516 in the data.\n",
      "There are 110 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 540 characters in the data.\n",
      "The lexical diversity is 0.536 in the data.\n",
      "There are 78 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 339 characters in the data.\n",
      "The lexical diversity is 0.577 in the data.\n",
      "There are 59 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 237 characters in the data.\n",
      "The lexical diversity is 0.729 in the data.\n",
      "There are 245 tokens in the data.\n",
      "There are 87 unique tokens in the data.\n",
      "There are 1244 characters in the data.\n",
      "The lexical diversity is 0.355 in the data.\n",
      "There are 69 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 348 characters in the data.\n",
      "The lexical diversity is 0.609 in the data.\n",
      "There are 34 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 143 characters in the data.\n",
      "The lexical diversity is 0.824 in the data.\n",
      "There are 170 tokens in the data.\n",
      "There are 124 unique tokens in the data.\n",
      "There are 803 characters in the data.\n",
      "The lexical diversity is 0.729 in the data.\n",
      "There are 209 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 1028 characters in the data.\n",
      "The lexical diversity is 0.344 in the data.\n",
      "There are 239 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 1026 characters in the data.\n",
      "The lexical diversity is 0.218 in the data.\n",
      "There are 101 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 484 characters in the data.\n",
      "The lexical diversity is 0.723 in the data.\n",
      "There are 59 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 239 characters in the data.\n",
      "The lexical diversity is 0.475 in the data.\n",
      "There are 118 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 508 characters in the data.\n",
      "The lexical diversity is 0.576 in the data.\n",
      "There are 130 tokens in the data.\n",
      "There are 79 unique tokens in the data.\n",
      "There are 666 characters in the data.\n",
      "The lexical diversity is 0.608 in the data.\n",
      "There are 25 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 133 characters in the data.\n",
      "The lexical diversity is 0.600 in the data.\n",
      "There are 108 tokens in the data.\n",
      "There are 88 unique tokens in the data.\n",
      "There are 521 characters in the data.\n",
      "The lexical diversity is 0.815 in the data.\n",
      "There are 149 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 655 characters in the data.\n",
      "The lexical diversity is 0.356 in the data.\n",
      "There are 106 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 483 characters in the data.\n",
      "The lexical diversity is 0.396 in the data.\n",
      "There are 54 tokens in the data.\n",
      "There are 24 unique tokens in the data.\n",
      "There are 280 characters in the data.\n",
      "The lexical diversity is 0.444 in the data.\n",
      "There are 106 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 616 characters in the data.\n",
      "The lexical diversity is 0.604 in the data.\n",
      "There are 108 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 483 characters in the data.\n",
      "The lexical diversity is 0.565 in the data.\n",
      "There are 146 tokens in the data.\n",
      "There are 84 unique tokens in the data.\n",
      "There are 715 characters in the data.\n",
      "The lexical diversity is 0.575 in the data.\n",
      "There are 121 tokens in the data.\n",
      "There are 90 unique tokens in the data.\n",
      "There are 565 characters in the data.\n",
      "The lexical diversity is 0.744 in the data.\n",
      "There are 67 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 319 characters in the data.\n",
      "The lexical diversity is 0.537 in the data.\n",
      "There are 109 tokens in the data.\n",
      "There are 84 unique tokens in the data.\n",
      "There are 525 characters in the data.\n",
      "The lexical diversity is 0.771 in the data.\n",
      "There are 120 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 603 characters in the data.\n",
      "The lexical diversity is 0.492 in the data.\n",
      "There are 121 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 646 characters in the data.\n",
      "The lexical diversity is 0.364 in the data.\n",
      "There are 93 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 471 characters in the data.\n",
      "The lexical diversity is 0.613 in the data.\n",
      "There are 57 tokens in the data.\n",
      "There are 32 unique tokens in the data.\n",
      "There are 278 characters in the data.\n",
      "The lexical diversity is 0.561 in the data.\n",
      "There are 80 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 403 characters in the data.\n",
      "The lexical diversity is 0.650 in the data.\n",
      "There are 89 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 433 characters in the data.\n",
      "The lexical diversity is 0.472 in the data.\n",
      "There are 104 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 472 characters in the data.\n",
      "The lexical diversity is 0.615 in the data.\n",
      "There are 112 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 510 characters in the data.\n",
      "The lexical diversity is 0.607 in the data.\n",
      "There are 101 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 481 characters in the data.\n",
      "The lexical diversity is 0.446 in the data.\n",
      "There are 157 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 705 characters in the data.\n",
      "The lexical diversity is 0.522 in the data.\n",
      "There are 88 tokens in the data.\n",
      "There are 39 unique tokens in the data.\n",
      "There are 439 characters in the data.\n",
      "The lexical diversity is 0.443 in the data.\n",
      "There are 93 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 488 characters in the data.\n",
      "The lexical diversity is 0.581 in the data.\n",
      "There are 85 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 367 characters in the data.\n",
      "The lexical diversity is 0.424 in the data.\n",
      "There are 77 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 323 characters in the data.\n",
      "The lexical diversity is 0.234 in the data.\n",
      "There are 197 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 877 characters in the data.\n",
      "The lexical diversity is 0.320 in the data.\n",
      "There are 68 tokens in the data.\n",
      "There are 34 unique tokens in the data.\n",
      "There are 296 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "There are 147 tokens in the data.\n",
      "There are 91 unique tokens in the data.\n",
      "There are 737 characters in the data.\n",
      "The lexical diversity is 0.619 in the data.\n",
      "There are 102 tokens in the data.\n",
      "There are 69 unique tokens in the data.\n",
      "There are 463 characters in the data.\n",
      "The lexical diversity is 0.676 in the data.\n",
      "There are 107 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 619 characters in the data.\n",
      "The lexical diversity is 0.402 in the data.\n",
      "There are 125 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 568 characters in the data.\n",
      "The lexical diversity is 0.520 in the data.\n",
      "There are 137 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 596 characters in the data.\n",
      "The lexical diversity is 0.445 in the data.\n",
      "There are 59 tokens in the data.\n",
      "There are 32 unique tokens in the data.\n",
      "There are 260 characters in the data.\n",
      "The lexical diversity is 0.542 in the data.\n",
      "There are 154 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 826 characters in the data.\n",
      "The lexical diversity is 0.370 in the data.\n",
      "There are 154 tokens in the data.\n",
      "There are 91 unique tokens in the data.\n",
      "There are 745 characters in the data.\n",
      "The lexical diversity is 0.591 in the data.\n",
      "There are 46 tokens in the data.\n",
      "There are 21 unique tokens in the data.\n",
      "There are 200 characters in the data.\n",
      "The lexical diversity is 0.457 in the data.\n",
      "There are 82 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 358 characters in the data.\n",
      "The lexical diversity is 0.573 in the data.\n",
      "There are 87 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 412 characters in the data.\n",
      "The lexical diversity is 0.724 in the data.\n",
      "There are 153 tokens in the data.\n",
      "There are 93 unique tokens in the data.\n",
      "There are 774 characters in the data.\n",
      "The lexical diversity is 0.608 in the data.\n",
      "There are 94 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 462 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "There are 116 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 589 characters in the data.\n",
      "The lexical diversity is 0.483 in the data.\n",
      "There are 122 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 611 characters in the data.\n",
      "The lexical diversity is 0.467 in the data.\n",
      "There are 138 tokens in the data.\n",
      "There are 99 unique tokens in the data.\n",
      "There are 740 characters in the data.\n",
      "The lexical diversity is 0.717 in the data.\n",
      "There are 102 tokens in the data.\n",
      "There are 25 unique tokens in the data.\n",
      "There are 435 characters in the data.\n",
      "The lexical diversity is 0.245 in the data.\n",
      "There are 132 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 569 characters in the data.\n",
      "The lexical diversity is 0.265 in the data.\n",
      "There are 135 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 662 characters in the data.\n",
      "The lexical diversity is 0.526 in the data.\n",
      "There are 115 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 488 characters in the data.\n",
      "The lexical diversity is 0.583 in the data.\n",
      "There are 177 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 835 characters in the data.\n",
      "The lexical diversity is 0.373 in the data.\n",
      "There are 156 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 687 characters in the data.\n",
      "The lexical diversity is 0.513 in the data.\n",
      "There are 63 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 316 characters in the data.\n",
      "The lexical diversity is 0.857 in the data.\n",
      "There are 87 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 434 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "There are 85 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 401 characters in the data.\n",
      "The lexical diversity is 0.471 in the data.\n",
      "There are 150 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 712 characters in the data.\n",
      "The lexical diversity is 0.413 in the data.\n",
      "There are 90 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 428 characters in the data.\n",
      "The lexical diversity is 0.533 in the data.\n",
      "There are 152 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 781 characters in the data.\n",
      "The lexical diversity is 0.395 in the data.\n",
      "There are 68 tokens in the data.\n",
      "There are 29 unique tokens in the data.\n",
      "There are 241 characters in the data.\n",
      "The lexical diversity is 0.426 in the data.\n",
      "There are 116 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 496 characters in the data.\n",
      "The lexical diversity is 0.543 in the data.\n",
      "There are 162 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 847 characters in the data.\n",
      "The lexical diversity is 0.469 in the data.\n",
      "There are 163 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 770 characters in the data.\n",
      "The lexical diversity is 0.417 in the data.\n",
      "There are 74 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 330 characters in the data.\n",
      "The lexical diversity is 0.635 in the data.\n",
      "There are 22 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 145 characters in the data.\n",
      "The lexical diversity is 0.455 in the data.\n",
      "There are 115 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 598 characters in the data.\n",
      "The lexical diversity is 0.565 in the data.\n",
      "There are 163 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 665 characters in the data.\n",
      "The lexical diversity is 0.350 in the data.\n",
      "There are 150 tokens in the data.\n",
      "There are 107 unique tokens in the data.\n",
      "There are 794 characters in the data.\n",
      "The lexical diversity is 0.713 in the data.\n",
      "There are 93 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 436 characters in the data.\n",
      "The lexical diversity is 0.430 in the data.\n",
      "There are 84 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 425 characters in the data.\n",
      "The lexical diversity is 0.655 in the data.\n",
      "There are 173 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 785 characters in the data.\n",
      "The lexical diversity is 0.324 in the data.\n",
      "There are 102 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 490 characters in the data.\n",
      "The lexical diversity is 0.569 in the data.\n",
      "There are 113 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 540 characters in the data.\n",
      "The lexical diversity is 0.398 in the data.\n",
      "There are 87 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 445 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "There are 59 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 267 characters in the data.\n",
      "The lexical diversity is 0.559 in the data.\n",
      "There are 94 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 388 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "There are 94 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 464 characters in the data.\n",
      "The lexical diversity is 0.479 in the data.\n",
      "There are 113 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 465 characters in the data.\n",
      "The lexical diversity is 0.496 in the data.\n",
      "There are 113 tokens in the data.\n",
      "There are 69 unique tokens in the data.\n",
      "There are 558 characters in the data.\n",
      "The lexical diversity is 0.611 in the data.\n",
      "There are 96 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 479 characters in the data.\n",
      "The lexical diversity is 0.562 in the data.\n",
      "There are 118 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 617 characters in the data.\n",
      "The lexical diversity is 0.619 in the data.\n",
      "There are 72 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 393 characters in the data.\n",
      "The lexical diversity is 0.861 in the data.\n",
      "There are 95 tokens in the data.\n",
      "There are 34 unique tokens in the data.\n",
      "There are 409 characters in the data.\n",
      "The lexical diversity is 0.358 in the data.\n",
      "There are 146 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 564 characters in the data.\n",
      "The lexical diversity is 0.308 in the data.\n",
      "There are 71 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 408 characters in the data.\n",
      "The lexical diversity is 0.775 in the data.\n",
      "There are 127 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 652 characters in the data.\n",
      "The lexical diversity is 0.339 in the data.\n",
      "There are 100 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 445 characters in the data.\n",
      "The lexical diversity is 0.530 in the data.\n",
      "There are 137 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 642 characters in the data.\n",
      "The lexical diversity is 0.540 in the data.\n",
      "There are 96 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 449 characters in the data.\n",
      "The lexical diversity is 0.490 in the data.\n",
      "There are 123 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 577 characters in the data.\n",
      "The lexical diversity is 0.512 in the data.\n",
      "There are 80 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 404 characters in the data.\n",
      "The lexical diversity is 0.725 in the data.\n",
      "There are 129 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 581 characters in the data.\n",
      "The lexical diversity is 0.271 in the data.\n",
      "There are 137 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 691 characters in the data.\n",
      "The lexical diversity is 0.299 in the data.\n",
      "There are 110 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 529 characters in the data.\n",
      "The lexical diversity is 0.645 in the data.\n",
      "There are 86 tokens in the data.\n",
      "There are 49 unique tokens in the data.\n",
      "There are 413 characters in the data.\n",
      "The lexical diversity is 0.570 in the data.\n",
      "There are 156 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 761 characters in the data.\n",
      "The lexical diversity is 0.340 in the data.\n",
      "There are 95 tokens in the data.\n",
      "There are 37 unique tokens in the data.\n",
      "There are 373 characters in the data.\n",
      "The lexical diversity is 0.389 in the data.\n",
      "There are 86 tokens in the data.\n",
      "There are 37 unique tokens in the data.\n",
      "There are 384 characters in the data.\n",
      "The lexical diversity is 0.430 in the data.\n",
      "There are 78 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 397 characters in the data.\n",
      "The lexical diversity is 0.872 in the data.\n",
      "There are 144 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 716 characters in the data.\n",
      "The lexical diversity is 0.438 in the data.\n",
      "There are 73 tokens in the data.\n",
      "There are 49 unique tokens in the data.\n",
      "There are 311 characters in the data.\n",
      "The lexical diversity is 0.671 in the data.\n",
      "There are 89 tokens in the data.\n",
      "There are 50 unique tokens in the data.\n",
      "There are 408 characters in the data.\n",
      "The lexical diversity is 0.562 in the data.\n",
      "There are 147 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 706 characters in the data.\n",
      "The lexical diversity is 0.497 in the data.\n",
      "There are 49 tokens in the data.\n",
      "There are 25 unique tokens in the data.\n",
      "There are 208 characters in the data.\n",
      "The lexical diversity is 0.510 in the data.\n",
      "There are 143 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 813 characters in the data.\n",
      "The lexical diversity is 0.580 in the data.\n",
      "There are 90 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 437 characters in the data.\n",
      "The lexical diversity is 0.444 in the data.\n",
      "There are 122 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 508 characters in the data.\n",
      "The lexical diversity is 0.426 in the data.\n",
      "There are 87 tokens in the data.\n",
      "There are 39 unique tokens in the data.\n",
      "There are 425 characters in the data.\n",
      "The lexical diversity is 0.448 in the data.\n",
      "There are 137 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 631 characters in the data.\n",
      "The lexical diversity is 0.511 in the data.\n",
      "There are 110 tokens in the data.\n",
      "There are 75 unique tokens in the data.\n",
      "There are 575 characters in the data.\n",
      "The lexical diversity is 0.682 in the data.\n",
      "There are 146 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 758 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "There are 183 tokens in the data.\n",
      "There are 85 unique tokens in the data.\n",
      "There are 959 characters in the data.\n",
      "The lexical diversity is 0.464 in the data.\n",
      "There are 167 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 862 characters in the data.\n",
      "The lexical diversity is 0.389 in the data.\n",
      "There are 220 tokens in the data.\n",
      "There are 106 unique tokens in the data.\n",
      "There are 965 characters in the data.\n",
      "The lexical diversity is 0.482 in the data.\n",
      "There are 131 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 500 characters in the data.\n",
      "The lexical diversity is 0.267 in the data.\n",
      "There are 100 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 474 characters in the data.\n",
      "The lexical diversity is 0.730 in the data.\n",
      "There are 159 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 692 characters in the data.\n",
      "The lexical diversity is 0.428 in the data.\n",
      "There are 148 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 719 characters in the data.\n",
      "The lexical diversity is 0.426 in the data.\n",
      "There are 76 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 424 characters in the data.\n",
      "The lexical diversity is 0.842 in the data.\n",
      "There are 156 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 727 characters in the data.\n",
      "The lexical diversity is 0.378 in the data.\n",
      "There are 131 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 659 characters in the data.\n",
      "The lexical diversity is 0.351 in the data.\n",
      "There are 148 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 723 characters in the data.\n",
      "The lexical diversity is 0.541 in the data.\n",
      "There are 84 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 439 characters in the data.\n",
      "The lexical diversity is 0.655 in the data.\n",
      "There are 125 tokens in the data.\n",
      "There are 88 unique tokens in the data.\n",
      "There are 577 characters in the data.\n",
      "The lexical diversity is 0.704 in the data.\n",
      "There are 169 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 811 characters in the data.\n",
      "The lexical diversity is 0.473 in the data.\n",
      "There are 165 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 694 characters in the data.\n",
      "The lexical diversity is 0.291 in the data.\n",
      "There are 123 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 603 characters in the data.\n",
      "The lexical diversity is 0.496 in the data.\n",
      "There are 178 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 885 characters in the data.\n",
      "The lexical diversity is 0.298 in the data.\n",
      "There are 50 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 240 characters in the data.\n",
      "The lexical diversity is 0.720 in the data.\n",
      "There are 72 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 345 characters in the data.\n",
      "The lexical diversity is 0.569 in the data.\n",
      "There are 102 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 497 characters in the data.\n",
      "The lexical diversity is 0.608 in the data.\n",
      "There are 62 tokens in the data.\n",
      "There are 23 unique tokens in the data.\n",
      "There are 273 characters in the data.\n",
      "The lexical diversity is 0.371 in the data.\n",
      "There are 129 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 643 characters in the data.\n",
      "The lexical diversity is 0.442 in the data.\n",
      "There are 71 tokens in the data.\n",
      "There are 30 unique tokens in the data.\n",
      "There are 280 characters in the data.\n",
      "The lexical diversity is 0.423 in the data.\n",
      "There are 136 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 688 characters in the data.\n",
      "The lexical diversity is 0.493 in the data.\n",
      "There are 108 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 534 characters in the data.\n",
      "The lexical diversity is 0.407 in the data.\n",
      "There are 102 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 501 characters in the data.\n",
      "The lexical diversity is 0.716 in the data.\n",
      "There are 131 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 639 characters in the data.\n",
      "The lexical diversity is 0.435 in the data.\n",
      "There are 120 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 581 characters in the data.\n",
      "The lexical diversity is 0.583 in the data.\n",
      "There are 74 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 378 characters in the data.\n",
      "The lexical diversity is 0.581 in the data.\n",
      "There are 91 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 497 characters in the data.\n",
      "The lexical diversity is 0.637 in the data.\n",
      "There are 93 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 437 characters in the data.\n",
      "The lexical diversity is 0.484 in the data.\n",
      "There are 116 tokens in the data.\n",
      "There are 49 unique tokens in the data.\n",
      "There are 526 characters in the data.\n",
      "The lexical diversity is 0.422 in the data.\n",
      "There are 134 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 678 characters in the data.\n",
      "The lexical diversity is 0.425 in the data.\n",
      "There are 141 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 616 characters in the data.\n",
      "The lexical diversity is 0.589 in the data.\n",
      "There are 87 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 354 characters in the data.\n",
      "The lexical diversity is 0.678 in the data.\n",
      "There are 185 tokens in the data.\n",
      "There are 106 unique tokens in the data.\n",
      "There are 880 characters in the data.\n",
      "The lexical diversity is 0.573 in the data.\n",
      "There are 122 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 411 characters in the data.\n",
      "The lexical diversity is 0.270 in the data.\n",
      "There are 83 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 358 characters in the data.\n",
      "The lexical diversity is 0.639 in the data.\n",
      "There are 99 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 455 characters in the data.\n",
      "The lexical diversity is 0.586 in the data.\n",
      "There are 122 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 591 characters in the data.\n",
      "The lexical diversity is 0.598 in the data.\n",
      "There are 75 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 399 characters in the data.\n",
      "The lexical diversity is 0.373 in the data.\n",
      "There are 59 tokens in the data.\n",
      "There are 38 unique tokens in the data.\n",
      "There are 272 characters in the data.\n",
      "The lexical diversity is 0.644 in the data.\n",
      "There are 70 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 338 characters in the data.\n",
      "The lexical diversity is 0.600 in the data.\n",
      "There are 114 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 521 characters in the data.\n",
      "The lexical diversity is 0.465 in the data.\n",
      "There are 115 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 467 characters in the data.\n",
      "The lexical diversity is 0.513 in the data.\n",
      "There are 109 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 493 characters in the data.\n",
      "The lexical diversity is 0.697 in the data.\n",
      "There are 108 tokens in the data.\n",
      "There are 84 unique tokens in the data.\n",
      "There are 544 characters in the data.\n",
      "The lexical diversity is 0.778 in the data.\n",
      "There are 126 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 557 characters in the data.\n",
      "The lexical diversity is 0.492 in the data.\n",
      "There are 84 tokens in the data.\n",
      "There are 29 unique tokens in the data.\n",
      "There are 366 characters in the data.\n",
      "The lexical diversity is 0.345 in the data.\n",
      "There are 102 tokens in the data.\n",
      "There are 81 unique tokens in the data.\n",
      "There are 562 characters in the data.\n",
      "The lexical diversity is 0.794 in the data.\n",
      "There are 77 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 397 characters in the data.\n",
      "The lexical diversity is 0.727 in the data.\n",
      "There are 79 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 370 characters in the data.\n",
      "The lexical diversity is 0.532 in the data.\n",
      "There are 127 tokens in the data.\n",
      "There are 98 unique tokens in the data.\n",
      "There are 675 characters in the data.\n",
      "The lexical diversity is 0.772 in the data.\n",
      "There are 70 tokens in the data.\n",
      "There are 22 unique tokens in the data.\n",
      "There are 256 characters in the data.\n",
      "The lexical diversity is 0.314 in the data.\n",
      "There are 93 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 457 characters in the data.\n",
      "The lexical diversity is 0.516 in the data.\n",
      "There are 113 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 506 characters in the data.\n",
      "The lexical diversity is 0.726 in the data.\n",
      "There are 131 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 596 characters in the data.\n",
      "The lexical diversity is 0.412 in the data.\n",
      "There are 234 tokens in the data.\n",
      "There are 129 unique tokens in the data.\n",
      "There are 1117 characters in the data.\n",
      "The lexical diversity is 0.551 in the data.\n",
      "There are 99 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 455 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "There are 135 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 637 characters in the data.\n",
      "The lexical diversity is 0.459 in the data.\n",
      "There are 106 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 605 characters in the data.\n",
      "The lexical diversity is 0.689 in the data.\n",
      "There are 115 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 509 characters in the data.\n",
      "The lexical diversity is 0.635 in the data.\n",
      "There are 77 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 391 characters in the data.\n",
      "The lexical diversity is 0.545 in the data.\n",
      "There are 122 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 537 characters in the data.\n",
      "The lexical diversity is 0.467 in the data.\n",
      "There are 119 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 539 characters in the data.\n",
      "The lexical diversity is 0.429 in the data.\n",
      "There are 231 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 1076 characters in the data.\n",
      "The lexical diversity is 0.316 in the data.\n",
      "There are 83 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 462 characters in the data.\n",
      "The lexical diversity is 0.578 in the data.\n",
      "There are 139 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 674 characters in the data.\n",
      "The lexical diversity is 0.489 in the data.\n",
      "There are 100 tokens in the data.\n",
      "There are 49 unique tokens in the data.\n",
      "There are 456 characters in the data.\n",
      "The lexical diversity is 0.490 in the data.\n",
      "There are 75 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 359 characters in the data.\n",
      "The lexical diversity is 0.640 in the data.\n",
      "There are 203 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 1031 characters in the data.\n",
      "The lexical diversity is 0.286 in the data.\n",
      "There are 94 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 461 characters in the data.\n",
      "The lexical diversity is 0.574 in the data.\n",
      "There are 117 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 551 characters in the data.\n",
      "The lexical diversity is 0.393 in the data.\n",
      "There are 75 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 400 characters in the data.\n",
      "The lexical diversity is 0.813 in the data.\n",
      "There are 110 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 500 characters in the data.\n",
      "The lexical diversity is 0.464 in the data.\n",
      "There are 118 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 638 characters in the data.\n",
      "The lexical diversity is 0.542 in the data.\n",
      "There are 150 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 707 characters in the data.\n",
      "The lexical diversity is 0.453 in the data.\n",
      "There are 233 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 987 characters in the data.\n",
      "The lexical diversity is 0.245 in the data.\n",
      "There are 132 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 651 characters in the data.\n",
      "The lexical diversity is 0.576 in the data.\n",
      "There are 128 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 592 characters in the data.\n",
      "The lexical diversity is 0.375 in the data.\n",
      "There are 131 tokens in the data.\n",
      "There are 78 unique tokens in the data.\n",
      "There are 648 characters in the data.\n",
      "The lexical diversity is 0.595 in the data.\n",
      "There are 66 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 344 characters in the data.\n",
      "The lexical diversity is 0.682 in the data.\n",
      "There are 144 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 683 characters in the data.\n",
      "The lexical diversity is 0.458 in the data.\n",
      "There are 172 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 858 characters in the data.\n",
      "The lexical diversity is 0.302 in the data.\n",
      "There are 112 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 562 characters in the data.\n",
      "The lexical diversity is 0.714 in the data.\n",
      "There are 78 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 357 characters in the data.\n",
      "The lexical diversity is 0.551 in the data.\n",
      "There are 202 tokens in the data.\n",
      "There are 77 unique tokens in the data.\n",
      "There are 892 characters in the data.\n",
      "The lexical diversity is 0.381 in the data.\n",
      "There are 119 tokens in the data.\n",
      "There are 69 unique tokens in the data.\n",
      "There are 526 characters in the data.\n",
      "The lexical diversity is 0.580 in the data.\n",
      "There are 110 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 498 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "There are 119 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 638 characters in the data.\n",
      "The lexical diversity is 0.437 in the data.\n",
      "There are 138 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 670 characters in the data.\n",
      "The lexical diversity is 0.529 in the data.\n",
      "There are 212 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 998 characters in the data.\n",
      "The lexical diversity is 0.340 in the data.\n",
      "There are 93 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 383 characters in the data.\n",
      "The lexical diversity is 0.441 in the data.\n",
      "There are 46 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 200 characters in the data.\n",
      "The lexical diversity is 0.783 in the data.\n",
      "There are 135 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 675 characters in the data.\n",
      "The lexical diversity is 0.459 in the data.\n",
      "There are 75 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 311 characters in the data.\n",
      "The lexical diversity is 0.680 in the data.\n"
     ]
    }
   ],
   "source": [
    "# calls to descriptive_stats here\n",
    "for text in lyrics_data['lyrics_clean']:\n",
    "    try:\n",
    "        tokens = text  # Add this line to get the tokens from the text\n",
    "        stats_verbose_true = descriptive_stats(text, verbose=True)\n",
    "        stats_verbose_false = descriptive_stats(text, verbose=False)\n",
    "        expected_num_tokens = len(tokens)\n",
    "        expected_num_unique_tokens = len(set(tokens))\n",
    "        expected_lexical_diversity = len(set(tokens)) / len(tokens)\n",
    "        expected_num_characters = sum(len(token) for token in tokens)\n",
    "        \n",
    "        assert(stats_verbose_true[0] == expected_num_tokens)\n",
    "        assert(stats_verbose_false[1] == expected_num_unique_tokens)\n",
    "        assert(abs(stats_verbose_false[2] - expected_lexical_diversity) < 0.02)\n",
    "        assert(stats_verbose_false[3] == expected_num_characters)\n",
    "    except AssertionError:\n",
    "        print(f\"Assertion failed for text: {text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74f0a227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text: []. Error: division by zero\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 122 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 52 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 45 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "Error processing text: []. Error: division by zero\n",
      "Error processing text: []. Error: division by zero\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 72 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 76 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 11 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 72 characters in the data.\n",
      "The lexical diversity is 0.909 in the data.\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 21 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 119 characters in the data.\n",
      "The lexical diversity is 0.905 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 114 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 105 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 70 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 13 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 60 characters in the data.\n",
      "The lexical diversity is 0.846 in the data.\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 104 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 51 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 68 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "Error processing text: []. Error: division by zero\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 58 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "Error processing text: []. Error: division by zero\n",
      "Error processing text: []. Error: division by zero\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 52 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 10 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 56 characters in the data.\n",
      "The lexical diversity is 0.700 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 45 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 81 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 81 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 61 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 13 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 69 characters in the data.\n",
      "The lexical diversity is 0.846 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 14 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 121 characters in the data.\n",
      "The lexical diversity is 0.929 in the data.\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 60 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 79 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "Error processing text: []. Error: division by zero\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "Error processing text: []. Error: division by zero\n",
      "Error processing text: []. Error: division by zero\n"
     ]
    }
   ],
   "source": [
    "## set a counter to only print out 100 values cause my computer crashed twice \n",
    "counter = 0\n",
    "for text in joined_twitter_data['cleaned_description']:\n",
    "    try:\n",
    "        tokens = text  # Add this line to get the tokens from the text\n",
    "        stats_verbose_true = descriptive_stats(text, verbose=True)\n",
    "        stats_verbose_false = descriptive_stats(text, verbose=False)\n",
    "        expected_num_tokens = len(tokens)\n",
    "        expected_num_unique_tokens = len(set(tokens))\n",
    "        expected_lexical_diversity = len(set(tokens)) / len(tokens)\n",
    "        expected_num_characters = sum(len(token) for token in tokens)\n",
    "        \n",
    "        assert(stats_verbose_true[0] == expected_num_tokens)\n",
    "        assert(stats_verbose_false[1] == expected_num_unique_tokens)\n",
    "        assert(abs(stats_verbose_false[2] - expected_lexical_diversity) < 0.02)\n",
    "        assert(stats_verbose_false[3] == expected_num_characters)\n",
    "    except AssertionError:\n",
    "        print(f\"Assertion failed for text: {text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}. Error: {e}\")\n",
    "    \n",
    "    counter += 1\n",
    "    if counter >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: If we left the stop words in we would see alot of filler words which we could ptoentially see as the top 5. When we look at a sentence or paragraph we don't look at words like a, but, the, they, is...etc. Those are just filler words to help make a sentence, we only care about important words to ully udnerstand the topic of the paragraph. For ex we can about, money, bitcoin, postive, negative, crash etc, these words have more impact to us on understanding on what is going on. \n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: so my prior belief of lexical diversity was the amount of unique words in a text, or in our ase in song lyrics. I know the above output does have artist names in them but the top have is robyn and teh bottom half is cher (based of the order of the df above). It is intersting to see that some songs by either have more lexical diversity than others. But this makes sense in the context of music. Why? Cause if you think about music alot of hit songs are the same phrases or words repeated over a catchy beat which cuts down on unique words, while other songs may have more unique words as it maybe more storytelling and descritpive. If teh song is closer to 0 then it has no less diversity in words and if its greateer than lets say .5 it has more dversity in lyrics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"❤️\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis 😁\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "269cd433",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'is_emoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Find all emojis in the description\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m description\u001b[38;5;241m.\u001b[39msplit():\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43memoji\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_emoji\u001b[49m(word):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# If the emoji is already in the dictionary, increment its count\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m emoji_counts:\n\u001b[1;32m     12\u001b[0m             emoji_counts[word] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'is_emoji'"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "emoji_counts = {}\n",
    "for description in joined_twitter_data['description']:\n",
    "    # Find all emojis in the description\n",
    "    for word in description.split():\n",
    "        if emoji.is_emoji(word):\n",
    "            # If the emoji is already in the dictionary, increment its count\n",
    "            if word in emoji_counts:\n",
    "                emoji_counts[word] += 1\n",
    "            # Otherwise, add it to the dictionary with a count of 1\n",
    "            else:\n",
    "                emoji_counts[word] = 1\n",
    "# Sort the emoji counts in descending order and get the top 10\n",
    "top_10_emojis = sorted(emoji_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Print the top 10 emojis and their counts\n",
    "for emoji, count in top_10_emojis:\n",
    "    print(f'{emoji}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10e74cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏳️‍🌈: 16412\n",
      "❤️: 15466\n",
      "♥: 11093\n",
      "❤: 10081\n",
      "✨: 9335\n",
      "🌈: 6212\n",
      "💙: 3817\n",
      "💜: 3728\n",
      "🇺🇸: 3703\n",
      "💕: 3394\n",
      "Top 10 emojis for cher:\n",
      "❤️: 14261\n",
      "🏳️‍🌈: 14002\n",
      "♥: 9805\n",
      "❤: 9338\n",
      "✨: 8342\n",
      "🌈: 5443\n",
      "💙: 3577\n",
      "🇺🇸: 3504\n",
      "💜: 3448\n",
      "💕: 3185\n",
      "Top 10 emojis for robynkonichiwa:\n",
      "🏳️‍🌈: 2410\n",
      "♥: 1288\n",
      "❤️: 1205\n",
      "✨: 993\n",
      "🌈: 769\n",
      "❤: 743\n",
      "🎶: 336\n",
      "🖤: 285\n",
      "💜: 280\n",
      "🎧: 258\n"
     ]
    }
   ],
   "source": [
    "emoji_counts = {}\n",
    "artist_emoji_counts = {}\n",
    "for index, row in joined_twitter_data.iterrows():\n",
    "    description = row['description']\n",
    "    artist_name = row['artist_name']\n",
    "    # Find all emojis in the description\n",
    "    for word in description.split():\n",
    "        if emoji.is_emoji(word):\n",
    "            # If the emoji is already in the dictionary, increment its count\n",
    "            if word in emoji_counts:\n",
    "                emoji_counts[word] += 1\n",
    "            # Otherwise, add it to the dictionary with a count of 1\n",
    "            else:\n",
    "                emoji_counts[word] = 1\n",
    "            # Also, add the emoji count to the artist's dictionary\n",
    "            if artist_name in artist_emoji_counts:\n",
    "                if word in artist_emoji_counts[artist_name]:\n",
    "                    artist_emoji_counts[artist_name][word] += 1\n",
    "                else:\n",
    "                    artist_emoji_counts[artist_name][word] = 1\n",
    "            else:\n",
    "                artist_emoji_counts[artist_name] = {word: 1}\n",
    "# Sort the emoji counts in descending order and get the top 10\n",
    "top_10_emojis = sorted(emoji_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Print the top 10 emojis and their counts\n",
    "for emoji, count in top_10_emojis:\n",
    "    print(f'{emoji}: {count}')\n",
    "\n",
    "# Also, print the top 10 emojis for each artist\n",
    "for artist, emoji_count in artist_emoji_counts.items():\n",
    "    top_10_emojis = sorted(emoji_count.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(f'Top 10 emojis for {artist}:')\n",
    "    for emoji, count in top_10_emojis:\n",
    "        print(f'{emoji}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#BLM: 9841\n",
      "#Resist: 6079\n",
      "#BlackLivesMatter: 5018\n",
      "#resist: 3825\n",
      "#FBR: 3252\n",
      "#TheResistance: 3013\n",
      "#blacklivesmatter: 2853\n",
      "#1: 2826\n",
      "#Resistance: 1929\n",
      "#LGBTQ: 1889\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "hashtag_counts = {}\n",
    "for description in twitter_df['description']:\n",
    "    # Find all hashtags in the description using regular expression\n",
    "    hashtags = re.findall(r'#\\w+', description)\n",
    "    \n",
    "    # Loop through each hashtag\n",
    "    for hashtag in hashtags:\n",
    "        # If the hashtag is already in the dictionary, increment its count\n",
    "        if hashtag in hashtag_counts:\n",
    "            hashtag_counts[hashtag] += 1\n",
    "        # Otherwise, add it to the dictionary with a count of 1\n",
    "        else:\n",
    "            hashtag_counts[hashtag] = 1\n",
    "\n",
    "# Sort the hashtag counts in descending order and get the top 10\n",
    "top_10_hashtags = sorted(hashtag_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Print the top 10 hashtags and their counts\n",
    "for hashtag, count in top_10_hashtags:\n",
    "    print(f'{hashtag}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa711308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 hashtags for cher:\n",
      "#BLM: 9682\n",
      "#Resist: 6065\n",
      "#BlackLivesMatter: 4866\n",
      "#resist: 3817\n",
      "#FBR: 3245\n",
      "#TheResistance: 3010\n",
      "#blacklivesmatter: 2749\n",
      "#1: 2692\n",
      "#Resistance: 1926\n",
      "#LGBTQ: 1864\n",
      "Top 10 hashtags for robynkonichiwa:\n",
      "#BlackLivesMatter: 523\n",
      "#BLM: 453\n",
      "#blacklivesmatter: 312\n",
      "#1: 264\n",
      "#music: 208\n",
      "#Music: 148\n",
      "#LGBTQ: 125\n",
      "#EDM: 98\n",
      "#blm: 78\n",
      "#Resist: 72\n"
     ]
    }
   ],
   "source": [
    "hashtag_counts = {}\n",
    "artist_hashtag_counts = {}\n",
    "for index, row in joined_twitter_data.iterrows():\n",
    "    description = row['description']\n",
    "    artist_name = row['artist_name']\n",
    "    # Find all hashtags in the description using regular expression\n",
    "    hashtags = re.findall(r'#\\w+', description)\n",
    "    \n",
    "    # Loop through each hashtag\n",
    "    for hashtag in hashtags:\n",
    "        # If the hashtag is already in the dictionary, increment its count\n",
    "        if hashtag in hashtag_counts:\n",
    "            hashtag_counts[hashtag] += 1\n",
    "        # Otherwise, add it to the dictionary with a count of 1\n",
    "        else:\n",
    "            hashtag_counts[hashtag] = 1\n",
    "        # Also, add the hashtag count to the artist's dictionary\n",
    "        if artist_name in artist_hashtag_counts:\n",
    "            if hashtag in artist_hashtag_counts[artist_name]:\n",
    "                artist_hashtag_counts[artist_name][hashtag] += 1\n",
    "            else:\n",
    "                artist_hashtag_counts[artist_name][hashtag] = 1\n",
    "        else:\n",
    "            artist_hashtag_counts[artist_name] = {hashtag: 1}\n",
    "\n",
    "# Sort the hashtag counts in descending order and get the top 10\n",
    "top_10_hashtags = sorted(hashtag_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# # Print the top 10 hashtags and their counts\n",
    "# for hashtag, count in top_10_hashtags:\n",
    "#     print(f'{hashtag}: {count}')\n",
    "\n",
    "# Also, print the top 10 hashtags for each artist\n",
    "for artist, hashtag_count in artist_hashtag_counts.items():\n",
    "    top_10_hashtags = sorted(hashtag_count.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(f'Top 10 hashtags for {artist}:')\n",
    "    for hashtag, count in top_10_hashtags:\n",
    "        print(f'{hashtag}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 36), ('To', 34), ('You', 25), ('\"The', 25), ('Of', 23)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "song_titles = []\n",
    "for artist in os.listdir(f'{data_location}/lyrics'):\n",
    "    artist_folder = os.path.join(data_location, 'lyrics', artist)\n",
    "    for song in os.listdir(artist_folder):\n",
    "        with open(os.path.join(artist_folder, song), 'r') as file:\n",
    "            # Read the first line of the file (song title)\n",
    "            song_title = file.readline().strip()\n",
    "            # Split the song title into words\n",
    "            words = song_title.split()\n",
    "            # Add the words to the list\n",
    "            song_titles.extend(words)\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_counts = Counter(song_titles)\n",
    "\n",
    "# Get the 5 most common words\n",
    "most_common_words = word_counts.most_common(5)\n",
    "\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bac5fd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAHHCAYAAABEEKc/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABdpUlEQVR4nO3deVxU1f8/8NeAwiDIgLIrAiqKC4KismhiSo1JJmmJZomES6W44PIRU0k/Jq5JLmXW56tmblmpZUoS7kqYiJq7KIWpA26Aouzn94c/bs2wOKPAAL6ej8c8mDn33Hvf93CBN+eee65MCCFARERERBIDfQdAREREVNMwQSIiIiLSwASJiIiISAMTJCIiIiINTJCIiIiINDBBIiIiItLABImIiIhIAxMkIiIiIg1MkIiIiIg0MEGi55KzszOGDx+u7zDqvEWLFqF58+YwNDSEp6envsOhf/nzzz8hk8mwePHiKtvH8OHDYWZmVmXbry4lbbV27Vp9h0LViAkS1Xpr166FTCbD8ePHy1zes2dPtG/f/pn3s2vXLnz00UfPvJ3nxZ49ezB16lR069YNa9aswbx58yqs/9NPP8Hf3x82NjZo0KABmjdvjkGDBiE2NraaIn46lXV+VZXn6bwtKiqCg4MDZDIZdu/erfP6GzduRExMTKXF89lnnzGpqsWYINFz6eLFi/jyyy91WmfXrl2YPXt2FUVU9+zduxcGBgb43//+h2HDhqFv377l1l28eDFee+01yGQyREZGYunSpRg4cCAuX76MzZs3V2PUdc/zdN7u3bsXN2/ehLOzMzZs2KDz+uUlSE5OTnj06BHeeecdnbbHBKl2q6fvAIj0wdjYWN8h6CwnJwempqb6DkNrGRkZMDExgZGRUYX1CgsL8d///hcvvfQS9uzZU+Z2iLTxzTffoFOnTggJCcH06dO1/pl5Uj2ZTAa5XF6ZoVItwB4kei5pjkEqKCjA7Nmz4erqCrlcjsaNG6N79+6Ii4sD8HgsxcqVKwE8/mVZ8iqRk5ODSZMmwdHREcbGxmjdujUWL14MIYTafh89eoRx48bBysoKDRs2xGuvvYbr169DJpOpXQb56KOPIJPJcO7cObz11luwtLRE9+7dAQCnT5/G8OHD0bx5c8jlctjZ2eHdd9/FnTt31PZVso1Lly7h7bffhkKhgLW1NWbOnAkhBK5du4b+/fvD3NwcdnZ2WLJkiVZtV5LQtGjRAsbGxnB2dsb06dORl5cn1ZHJZFizZg1ycnKktirvP+nbt28jOzsb3bp1K3O5jY2N2ueMjAyEhYXB1tYWcrkcHh4eWLdunVqdf4+vWb16tRRrly5d8Pvvv5fax9atW9G2bVvI5XK0b98e27Ztw/Dhw+Hs7KxVm2hj9+7deOGFF2BqaoqGDRsiMDAQZ8+eVatTMmbn+vXrCAoKgpmZGaytrTF58mQUFRWp1b1z5w7eeecdmJubw8LCAiEhITh16pRaWz/pvC3xpDZSqVQIDQ1F06ZNYWxsDHt7e/Tv3x9//vmnVsd+9epVKJVKmJqawsHBAXPmzJF+NoQQcHZ2Rv/+/Uutl5ubC4VCgdGjRz9xH48ePcK2bdswePBgDBo0CI8ePcKOHTtK1Stp4ytXrqBv375o2LAhhg4dip49e+Lnn3/GX3/9JbVTyfe/rDFIT2oTZ2dnnD17FgcOHJC217NnT63ai2oG9iBRnZGVlYXbt2+XKi8oKHjiuh999BGio6MxYsQIdO3aFdnZ2Th+/DhOnDiBl156CaNHj8aNGzcQFxeH9evXq60rhMBrr72Gffv2ISwsDJ6envjll18wZcoUXL9+HUuXLpXqDh8+HN9++y3eeecd+Pj44MCBAwgMDCw3rjfffBOurq6YN2+e9AclLi4OV69eRWhoKOzs7HD27FmsXr0aZ8+exW+//VbqD2BwcDDatGmD+fPn4+eff8bcuXPRqFEjfPHFF+jVqxcWLFiADRs2YPLkyejSpQt69OhRYVuNGDEC69atwxtvvIFJkyYhMTER0dHROH/+PLZt2wYAWL9+PVavXo1jx47hq6++AgD4+fmVuT0bGxuYmJjgp59+Qnh4OBo1alTuvh89eoSePXsiJSUFY8eOhYuLC7Zu3Yrhw4cjMzMT48ePV6u/ceNG3L9/H6NHj4ZMJsPChQsxYMAAXL16FfXr1wcA/PzzzwgODoa7uzuio6Nx7949hIWFoUmTJhW2gy7Wr1+PkJAQKJVKLFiwAA8fPsTnn3+O7t27Izk5WS0RKyoqglKphLe3NxYvXoxff/0VS5YsQYsWLfD+++8DAIqLi9GvXz8cO3YM77//Ptzc3LBjxw6EhISo7bei81aXNho4cCDOnj2L8PBwODs7IyMjA3FxcUhLS3tiEllUVIQ+ffrAx8cHCxcuRGxsLKKiolBYWIg5c+ZAJpPh7bffxsKFC3H37l217/9PP/2E7OxsvP32209s4x9//BEPHjzA4MGDYWdnh549e2LDhg146623StUtLCyEUqlE9+7dsXjxYjRo0AB2dnbIysrC33//Lf3MVjTA/EltEhMTg/DwcJiZmeHDDz8EANja2j7xOKgGEUS13Jo1awSACl/t2rVTW8fJyUmEhIRInz08PERgYGCF+xkzZowo60dm+/btAoCYO3euWvkbb7whZDKZSElJEUIIkZSUJACICRMmqNUbPny4ACCioqKksqioKAFADBkypNT+Hj58WKps06ZNAoA4ePBgqW2MGjVKKissLBRNmzYVMplMzJ8/Xyq/d++eMDExUWuTspw8eVIAECNGjFArnzx5sgAg9u7dK5WFhIQIU1PTCrdXYtasWQKAMDU1Fa+88or4+OOPRVJSUql6MTExAoD45ptvpLL8/Hzh6+srzMzMRHZ2thBCiNTUVAFANG7cWNy9e1equ2PHDgFA/PTTT1KZu7u7aNq0qbh//75Utn//fgFAODk5PTF2f3//UufXv92/f19YWFiIkSNHqpWrVCqhUCjUykNCQgQAMWfOHLW6HTt2FF5eXtLn77//XgAQMTExUllRUZHo1auXACDWrFkjlZd33mrbRvfu3RMAxKJFi57QEqWVHE94eLhUVlxcLAIDA4WRkZG4deuWEEKIixcvCgDi888/V1v/tddeE87OzqK4uPiJ+3r11VdFt27dpM+rV68W9erVExkZGWXGNG3atFLbCAwMLPN7XtJWJe2qbZu0a9dO+Pv7PzF2qpl4iY3qjJUrVyIuLq7Uq0OHDk9c18LCAmfPnsXly5d13u+uXbtgaGiIcePGqZVPmjQJQgjpbpqSu7E++OADtXrh4eHlbvu9994rVWZiYiK9z83Nxe3bt+Hj4wMAOHHiRKn6I0aMkN4bGhqic+fOEEIgLCxMKrewsEDr1q1x9erVcmMBHh8rAERERKiVT5o0CcDj3pinMXv2bGzcuBEdO3bEL7/8gg8//BBeXl7o1KkTzp8/r7Z/Ozs7DBkyRCqrX78+xo0bhwcPHuDAgQNq2w0ODoalpaX0+YUXXgAA6Thv3LiBP/74A8OGDVPrLfD394e7u/tTHYumuLg4ZGZmYsiQIbh9+7b0MjQ0hLe3N/bt21dqHc3v+wsvvKD2vYmNjUX9+vUxcuRIqczAwABjxozROb4ntVHJOLL9+/fj3r17Om8fAMaOHSu9l8lkGDt2LPLz8/Hrr78CAFq1agVvb2+1gdV3797F7t27MXTo0DIvC/7bnTt38Msvv6idFwMHDoRMJsO3335b5jolvXFPozLahGo+JkhUZ3Tt2hUBAQGlXv/+5V+eOXPmIDMzE61atYK7uzumTJmC06dPa7Xfv/76Cw4ODmjYsKFaeZs2baTlJV8NDAzg4uKiVq9ly5blbluzLvD4D8f48eNha2sLExMTWFtbS/WysrJK1W/WrJnaZ4VCAblcDisrq1LlT/plX3IMmjHb2dnBwsJCOtanMWTIEBw6dAj37t3Dnj178NZbbyE5ORn9+vVDbm6utH9XV1cYGKj/6tJs6xKax15yLpQcZ0n9sr4HFX1fdFGSdPfq1QvW1tZqrz179pQahC6Xy2FtbV0q7n9/b/766y/Y29ujQYMGzxzzk9rI2NgYCxYswO7du2Fra4sePXpg4cKFUKlUWm3fwMAAzZs3Vytr1aoVAKiNYRo2bBiOHDkifU+2bt2KgoICre4c27JlCwoKCtCxY0ekpKQgJSUFd+/eLZV0lahXrx6aNm2qVfxledY2odqBCRIRgB49euDKlSv4v//7P7Rv3x5fffUVOnXqJI2f0Zd/9xaVGDRoEL788ku89957+OGHH7Bnzx6pd6q4uLhUfUNDQ63KAJQaVF6eJ/1H/yzMzc3x0ksvYcOGDQgJCcGVK1eQmJj4VNt61uOsDCXfk/Xr15fZw6k5kLi8mKuKNm00YcIEXLp0CdHR0ZDL5Zg5cybatGmD5OTkSotj8ODBqF+/vpTQfPPNN+jcuTNat279xHVL1unWrRtcXV2l1+HDh5GQkFCqZ9TY2LhUkq2r6mgT0i8mSET/X6NGjRAaGopNmzbh2rVr6NChg9qdZeUlBU5OTrhx4wbu37+vVn7hwgVpecnX4uJipKamqtVLSUnROsZ79+4hPj4e06ZNw+zZs/H666/jpZdeKvUfelUpOQbNS5Hp6enIzMyUjrWydO7cGQBw8+ZNaf+XL18ulQhqtrW2SuqX9T3Q5ftSkRYtWgB4PBi9rB7Op7mzycnJCTdv3sTDhw/VysuKubKS2RYtWmDSpEnYs2cPzpw5g/z8fK3ufCwuLi6VoFy6dAkA1AZ4N2rUCIGBgdiwYQP++usvHDlyRKveo9TUVBw9ehRjx47F1q1b1V5btmyBkZERNm7cqNUx6tpWT2qTqvxHgqoeEyQioNQt8mZmZmjZsqXaresl86RkZmaq1e3bty+KioqwYsUKtfKlS5dCJpPhlVdeAQAolUoAjyeP+7fly5drHWfJf/uaPSCVOftvRUome9Tc3yeffAIAFd6RV56HDx8iISGhzGUl47dKehH69u0LlUqFLVu2SHUKCwuxfPlymJmZwd/fX6d9Ozg4oH379vj666/x4MEDqfzAgQP4448/dD2UMimVSpibm2PevHll3lF569atp9pmQUGB2mSnxcXF0i39/1beeauthw8fSpc4S7Ro0QINGzZU+/moyL9/NoQQWLFiBerXr4/evXur1XvnnXdw7tw5TJkyBYaGhhg8ePATt13SezR16lS88cYbaq9BgwbB399f60kjTU1Ny7xMrUnbNjE1NX3qdif9423+RADatm2Lnj17wsvLC40aNcLx48fx3XffqQ0u9fLyAgCMGzcOSqVS+gXer18/vPjii/jwww/x559/wsPDA3v27MGOHTswYcIEqQfBy8sLAwcORExMDO7cuSPd5l/y37Q2/22am5tL4x0KCgrQpEkT7Nmzp1SvVFXx8PBASEgIVq9ejczMTPj7++PYsWNYt24dgoKC8OKLL+q8zYcPH8LPzw8+Pj7o06cPHB0dkZmZie3bt+PQoUMICgpCx44dAQCjRo3CF198geHDhyMpKQnOzs747rvvcOTIEcTExJQaB6aNefPmoX///ujWrRtCQ0Nx7949rFixAu3bt1dLmipy69YtzJ07t1S5i4sLhg4dis8//xzvvPMOOnXqhMGDB8Pa2hppaWn4+eef0a1bt1LJ9ZMEBQWha9eumDRpElJSUuDm5oYff/wRd+/eBaB+LpV33mrr0qVL6N27NwYNGoS2bduiXr162LZtG9LT07XajlwuR2xsLEJCQuDt7Y3du3fj559/xvTp00uNtQoMDETjxo2xdetWvPLKK6XmwCrLhg0b4OnpCUdHxzKXv/baawgPD8eJEyfQqVOnCrfl5eWFLVu2ICIiAl26dIGZmRn69etXqp62beLl5YXPP/8cc+fORcuWLWFjY4NevXo98ZiohtDjHXRElaLkNv/ff/+9zOVl3YateZv/3LlzRdeuXYWFhYUwMTERbm5u4uOPPxb5+flSncLCQhEeHi6sra2FTCZTu3X6/v37YuLEicLBwUHUr19fuLq6ikWLFpW6PTknJ0eMGTNGNGrUSJiZmYmgoCDpFud/33Zfcot+yW3Q//b333+L119/XVhYWAiFQiHefPNNcePGjXKnCtDcRnm33z/pdvUSBQUFYvbs2cLFxUXUr19fODo6isjISJGbm6vVfsra3pdffimCgoKEk5OTMDY2Fg0aNBAdO3YUixYtEnl5eWr109PTRWhoqLCyshJGRkbC3d1d7bZ2If65Lbus27A120kIITZv3izc3NyEsbGxaN++vfjxxx/FwIEDhZub2xPj9/f3L3d6id69e0v19u3bJ5RKpVAoFEIul4sWLVqI4cOHi+PHjz+xzUq+l/9269Yt8dZbb4mGDRsKhUIhhg8fLo4cOSIAiM2bN0v1yjtvtW2j27dvizFjxgg3NzdhamoqFAqF8Pb2Ft9+++0T26bkeK5cuSJefvll0aBBA2FrayuioqJEUVFRmet88MEHAoDYuHHjE7dfMnXGzJkzy63z559/CgBi4sSJajGV5cGDB+Ktt94SFhYWatM8aN7mr22bqFQqERgYKBo2bCgA8Jb/WkYmRDWOViSiUk6ePImOHTvim2++wdChQ/UdDv1/np6esLa2lmZTrw22b9+O119/HYcPHy53ZvKabuLEifjf//4HlUpV6i49ourEMUhE1ejRo0elymJiYmBgYPDEGaypahQUFKCwsFCtbP/+/Th16lSNfjSE5rlUVFSE5cuXw9zc/ImXkmqq3NxcfPPNNxg4cCCTI9I7jkEiqkYLFy5EUlISXnzxRdSrVw+7d+/G7t27MWrUqHLHUFDVun79OgICAvD222/DwcEBFy5cwKpVq2BnZ1fmRJ01RXh4OB49egRfX1/k5eXhhx9+wNGjRzFv3rwyp4eoyTIyMvDrr7/iu+++w507d0o9MoZIH5ggEVUjPz8/xMXF4b///S8ePHiAZs2a4aOPPpKe1UTVz9LSEl5eXvjqq69w69YtmJqaIjAwEPPnz0fjxo31HV65evXqhSVLlmDnzp3Izc1Fy5YtsXz5crUbC2qLc+fOYejQobCxscGyZcvg6emp75CIwDFIRERERBo4BomIiIhIAxMkIiIiIg0cg/SUiouLcePGDTRs2JDTyRMREdUSQgjcv38fDg4OFT6TjwnSU7px4wbvOiIiIqqlrl27hqZNm5a7nAnSUyp5pMG1a9dgbm6u52iIiIhIG9nZ2XB0dHzio4mYID2lkstq5ubmTJCIiIhqmScNj+EgbSIiIiINTJCIiIiINDBBIiIiItKg9wRp5cqVcHZ2hlwuh7e3N44dO1Zh/a1bt8LNzQ1yuRzu7u7YtWuX2vIffvgBL7/8Mho3bgyZTIaTJ0+W2kZubi7GjBmDxo0bw8zMDAMHDkR6enplHhYREVGFioqKkJuby1clv4qKiirl+6PXQdpbtmxBREQEVq1aBW9vb8TExECpVOLixYuwsbEpVf/o0aMYMmQIoqOj8eqrr2Ljxo0ICgrCiRMn0L59ewBATk4OunfvjkGDBmHkyJFl7nfixIn4+eefsXXrVigUCowdOxYDBgzAkSNHqvR4iYiIhBBQqVTIzMzUdyh1loWFBezs7J5pnkK9PovN29sbXbp0wYoVKwA8nnzR0dER4eHhmDZtWqn6wcHByMnJwc6dO6UyHx8feHp6YtWqVWp1//zzT7i4uCA5OVntwYdZWVmwtrbGxo0b8cYbbwAALly4gDZt2iAhIQE+Pj5axZ6dnQ2FQoGsrCzexUZERFq7efMmMjMzYWNjgwYNGnCy4UokhMDDhw+RkZEBCwsL2Nvbl6qj7d9vvfUg5efnIykpCZGRkVKZgYEBAgICkJCQUOY6CQkJiIiIUCtTKpXYvn271vtNSkpCQUEBAgICpDI3Nzc0a9aswgQpLy8PeXl50ufs7Gyt90lERAQ8vqxWkhw1btxY3+HUSSYmJgCAjIwM2NjYwNDQ8Km2o7cxSLdv30ZRURFsbW3Vym1tbaFSqcpcR6VS6VS/vG0YGRnBwsJCp+1ER0dDoVBIL86iTUREuiooKAAANGjQQM+R1G0l7VvS3k9D74O0a4vIyEhkZWVJr2vXruk7JCIiqqV4Wa1qVUb76u0Sm5WVFQwNDUvdPZaeng47O7sy17Gzs9OpfnnbyM/PR2Zmplov0pO2Y2xsDGNjY633Q0RERLWX3nqQjIyM4OXlhfj4eKmsuLgY8fHx8PX1LXMdX19ftfoAEBcXV279snh5eaF+/fpq27l48SLS0tJ02g4RERE9vimqvGl1ajO93uYfERGBkJAQdO7cGV27dkVMTAxycnIQGhoKABg2bBiaNGmC6OhoAMD48ePh7++PJUuWIDAwEJs3b8bx48exevVqaZt3795FWloabty4AeBx8gM87jmys7ODQqFAWFgYIiIi0KhRI5ibmyM8PBy+vr5a38FGRERU2ZbGXaq2fU18qVW17au20muCFBwcjFu3bmHWrFlQqVTw9PREbGysNBA7LS0NBgb/dHL5+flh48aNmDFjBqZPnw5XV1ds375dmgMJAH788UcpwQKAwYMHAwCioqLw0UcfAQCWLl0KAwMDDBw4EHl5eVAqlfjss8+q4YiJiIhIG/n5+TAyMtLb/vU6D1JtxnmQiIhIV7m5uUhNTYWLiwvkcrnaspreg1RcXIzFixdj9erVuHbtGmxtbTF69GgMHToULi4u+P7777F8+XIkJibC1dUVq1atUhu6cvjwYURGRuL48eOwsrLC66+/jujoaJiamgIAnJ2dERYWhsuXL2P79u0YMGAA1q5d+1THV1E7a/v3m3exERER0RNFRkZi/vz5mDlzJs6dO4eNGzeqTb3z4YcfYvLkyTh58iRatWqFIUOGoLCwEABw5coV9OnTBwMHDsTp06exZcsWHD58GGPHjlXbx+LFi+Hh4YHk5GTMnDmzWo9PE3uQnhJ7kGqG6vyP6994/Z6InkZt7UG6f/8+rK2tsWLFCowYMUJtWcmTK7766iuEhYUBAM6dO4d27drh/PnzcHNzw4gRI2BoaIgvvvhCWu/w4cPw9/dHTk4O5HI5nJ2d0bFjR2zbtu2Zj489SERERFTlzp8/j7y8PPTu3bvcOh06dJDelzziIyMjAwBw6tQprF27FmZmZtJLqVSiuLgYqamp0nqdO3euoiPQnV4HaRMREVHNV/L4jorUr19fel8yUWNxcTEA4MGDBxg9ejTGjRtXar1mzZpJ70vGI9UETJCIiIioQq6urjAxMUF8fHypS2za6NSpE86dO4eWLVtWQXRVgwkSERERVUgul+M///kPpk6dCiMjI3Tr1g23bt3C2bNnK7zsVuI///kPfHx8MHbsWIwYMQKmpqY4d+4c4uLisGLFimo4At0xQSIiIqInmjlzJurVq4dZs2bhxo0bsLe3x3vvvafVuh06dMCBAwfw4Ycf4oUXXoAQAi1atEBwcHAVR/30eBfbU+JdbDUD72IjotqkorurqPLwLjYiIiKiKsAEiYiIiEgDEyQiIiIiDUyQiIiIiDQwQSIiIiLSwASJiIiISAMTJCIiIiINTJCIiIiINHAmbaKnoI8JKjk5JRFR9WEPEhEREVWJnj17YsKECfoO46mwB4mIiKgm2Bddfft6MbL69lVLsQeJiIiIdJafn6/vEKoUEyQiIiJ6op49e2Ls2LGYMGECrKysoFQqceDAAXTt2hXGxsawt7fHtGnTUFhYqLZeYWEhxo4dC4VCASsrK8ycORNCCADAnDlz0L59+1L78vT0xMyZMwEAw4cPR1BQEBYvXgx7e3s0btwYY8aMQUFBQZUeLy+xEdUS+hgYDnBwOBH9Y926dXj//fdx5MgRqFQq9O3bF8OHD8fXX3+NCxcuYOTIkZDL5fjoo4/U1gkLC8OxY8dw/PhxjBo1Cs2aNcPIkSPx7rvvYvbs2fj999/RpUsXAEBycjJOnz6NH374QdrGvn37YG9vj3379iElJQXBwcHw9PTEyJEjq+xYmSARERGRVlxdXbFw4UIAwNdffw1HR0esWLECMpkMbm5uuHHjBv7zn/9g1qxZMDB4fJHK0dERS5cuhUwmQ+vWrfHHH39g6dKlGDlyJJo2bQqlUok1a9ZICdKaNWvg7++P5s2bS/u1tLTEihUrYGhoCDc3NwQGBiI+Pr5KEyReYiMiIiKteHl5Se/Pnz8PX19fyGQyqaxbt2548OAB/v77b6nMx8dHrY6vry8uX76MoqIiAMDIkSOxadMm5ObmIj8/Hxs3bsS7776rtt927drB0NBQ+mxvb4+MjIxKP75/Yw8SERERacXU1LTSt9mvXz8YGxtj27ZtMDIyQkFBAd544w21OvXr11f7LJPJUFxcXOmx/BsTJCIiItJZmzZt8P3330MIIfUQHTlyBA0bNkTTpk2leomJiWrr/fbbb3B1dZV6hOrVq4eQkBCsWbMGRkZGGDx4MExMTKrvQMrBS2xERESksw8++ADXrl1DeHg4Lly4gB07diAqKgoRERHS+CMASEtLQ0REBC5evIhNmzZh+fLlGD9+vNq2RowYgb179yI2NrbU5TV9YQ8SERER6axJkybYtWsXpkyZAg8PDzRq1AhhYWGYMWOGWr1hw4bh0aNH6Nq1KwwNDTF+/HiMGjVKrY6rqyv8/Pxw9+5deHt7V+dhlEsmSiYjIJ1kZ2dDoVAgKysL5ubm+g7nuaWvW9+fJ7zNn6jy5ObmIjU1FS4uLpDL5foOp8YQQsDV1RUffPABIiIinnl7FbWztn+/2YNEREREenPr1i1s3rwZKpUKoaGh+g5HwgSJiIiI9MbGxgZWVlZYvXo1LC0t9R2OhAkSERER6U1NHenDu9iIiIiINDBBIiIiqmY1tdekrqiM9mWCREREVE1KZoR++PChniOp20raV3MGbl1wDBIREVE1MTQ0hIWFhfQcsQYNGqg9p4yejRACDx8+REZGBiwsLNSe36YrJkhERETVyM7ODgCq/GGrzzMLCwupnZ8WEyQiIqJqJJPJYG9vDxsbGxQUFOg7nDqnfv36z9RzVIIJEhERkR4YGhpWyh9yqhocpE1ERESkgQkSERERkQYmSEREREQamCARERERaWCCRERERKSBCRIRERGRBiZIRERERBqYIBERERFpYIJEREREpIEJEhEREZEGJkhEREREGpggEREREWlggkRERESkgQkSERERkQYmSEREREQamCARERERaWCCRERERKSBCRIRERGRBiZIRERERBqYIBERERFpYIJEREREpIEJEhEREZEGvSdIK1euhLOzM+RyOby9vXHs2LEK62/duhVubm6Qy+Vwd3fHrl271JYLITBr1izY29vDxMQEAQEBuHz5slqdS5cuoX///rCysoK5uTm6d++Offv2VfqxERERUe2k1wRpy5YtiIiIQFRUFE6cOAEPDw8olUpkZGSUWf/o0aMYMmQIwsLCkJycjKCgIAQFBeHMmTNSnYULF2LZsmVYtWoVEhMTYWpqCqVSidzcXKnOq6++isLCQuzduxdJSUnw8PDAq6++CpVKVeXHTERERDWfTAgh9LVzb29vdOnSBStWrAAAFBcXw9HREeHh4Zg2bVqp+sHBwcjJycHOnTulMh8fH3h6emLVqlUQQsDBwQGTJk3C5MmTAQBZWVmwtbXF2rVrMXjwYNy+fRvW1tY4ePAgXnjhBQDA/fv3YW5ujri4OAQEBGgVe3Z2NhQKBbKysmBubv6sTUFPaWncJX2HUOdNfKmVvkMgIqo02v791lsPUn5+PpKSktQSEgMDAwQEBCAhIaHMdRISEkolMEqlUqqfmpoKlUqlVkehUMDb21uq07hxY7Ru3Rpff/01cnJyUFhYiC+++AI2Njbw8vIqN968vDxkZ2ervYiIiKhu0luCdPv2bRQVFcHW1lat3NbWttxLXSqVqsL6JV8rqiOTyfDrr78iOTkZDRs2hFwuxyeffILY2FhYWlqWG290dDQUCoX0cnR01O2AiYiIqNbQ+yDt6iaEwJgxY2BjY4NDhw7h2LFjCAoKQr9+/XDz5s1y14uMjERWVpb0unbtWjVGTURERNVJbwmSlZUVDA0NkZ6erlaenp4OOzu7Mtexs7OrsH7J14rq7N27Fzt37sTmzZvRrVs3dOrUCZ999hlMTEywbt26cuM1NjaGubm52ouIiIjqJr0lSEZGRvDy8kJ8fLxUVlxcjPj4ePj6+pa5jq+vr1p9AIiLi5Pqu7i4wM7OTq1OdnY2EhMTpToPHz4E8Hi8078ZGBiguLj42Q+MiIiIar16+tx5REQEQkJC0LlzZ3Tt2hUxMTHIyclBaGgoAGDYsGFo0qQJoqOjAQDjx4+Hv78/lixZgsDAQGzevBnHjx/H6tWrATweXzRhwgTMnTsXrq6ucHFxwcyZM+Hg4ICgoCAAj5MsS0tLhISEYNasWTAxMcGXX36J1NRUBAYG6qUdiIiIqGbRa4IUHByMW7duYdasWVCpVPD09ERsbKw0yDotLU2tp8fPzw8bN27EjBkzMH36dLi6umL79u1o3769VGfq1KnIycnBqFGjkJmZie7duyM2NhZyuRzA40t7sbGx+PDDD9GrVy8UFBSgXbt22LFjBzw8PKq3AYiIiKhG0us8SLUZ50GqGTgPUtXjPEhEVJfU+HmQiIiIiGoqJkhEREREGpggEREREWlggkRERESkgQkSERERkQYmSEREREQamCARERERaWCCRERERKSBCRIRERGRBiZIRERERBqYIBERERFpYIJEREREpIEJEhEREZEGJkhEREREGpggEREREWlggkRERESkgQkSERERkQYmSEREREQamCARERERaWCCRERERKSBCRIRERGRBiZIRERERBqYIBERERFpYIJEREREpIEJEhEREZEGJkhEREREGpggEREREWmop+8AiPTJJ221vkPQ2W/NRuk7BCKiOo89SEREREQamCARERERaWCCRERERKSBCRIRERGRBp0TpDVr1uDhw4dVEQsRERFRjaBzgjRt2jTY2dkhLCwMR48erYqYiIiIiPRK5wTp+vXrWLduHW7fvo2ePXvCzc0NCxYsgEqlqor4iIiIiKqdzglSvXr18Prrr2PHjh24du0aRo4ciQ0bNqBZs2Z47bXXsGPHDhQXF1dFrERERETV4pkmirS1tUX37t1x6dIlXLp0CX/88QdCQkJgaWmJNWvWoGfPnpUUJhHpy9K4S9W+z4kvtar2fRIR/dtT3cWWnp6OxYsXo127dujZsyeys7Oxc+dOpKam4vr16xg0aBBCQkIqO1YiIiKiaqFzgtSvXz84Ojpi7dq1GDlyJK5fv45NmzYhICAAAGBqaopJkybh2rVrlR4sERERUXXQ+RKbjY0NDhw4AF9f33LrWFtbIzU19ZkCIyIiItIXnXuQ/P390alTp1Ll+fn5+PrrrwEAMpkMTk5Ozx4dERERkR7onCCFhoYiKyurVPn9+/cRGhpaKUERERER6ZPOCZIQAjKZrFT533//DYVCUSlBEREREemT1mOQOnbsCJlMBplMht69e6NevX9WLSoqQmpqKvr06VMlQRIRERFVJ60TpKCgIADAyZMnoVQqYWZmJi0zMjKCs7MzBg4cWOkBEhEREVU3rROkqKgoAICzszOCg4Mhl8urLCgiIiIifdL5Nn9OAElERER1nVYJUqNGjXDp0iVYWVnB0tKyzEHaJe7evVtpwRERERHpg1YJ0tKlS9GwYUPpfUUJEhEREVFtp1WC9O/LasOHD6+qWIiIiIhqBJ3nQTpx4gT++OMP6fOOHTsQFBSE6dOnIz8/v1KDIyIiItIHnROk0aNH49KlSwCAq1evIjg4GA0aNMDWrVsxderUSg+QiIiIqLrpnCBdunQJnp6eAICtW7fC398fGzduxNq1a/H9999XdnxERERE1e6pHjVSXFwMAPj111/Rt29fAICjoyNu375dudERERER6YHOCVLnzp0xd+5crF+/HgcOHEBgYCAAIDU1Fba2tpUeIBEREVF10zlBiomJwYkTJzB27Fh8+OGHaNmyJQDgu+++g5+fX6UHSERERFTddJ5Ju0OHDmp3sZVYtGgRDA0NKyUoIiIiIn3SOUEqkZ+fj4yMDGk8UolmzZo9c1BERERE+qRzgnTp0iWEhYXh6NGjauVCCMhkMhQVFVVacERERET6oHOCFBoainr16mHnzp2wt7fnY0eIiIioztE5QTp58iSSkpLg5uZWFfEQERER6Z3Od7G1bdu2Uuc7WrlyJZydnSGXy+Ht7Y1jx45VWH/r1q1wc3ODXC6Hu7s7du3apbZcCIFZs2bB3t4eJiYmCAgIwOXLl0tt5+eff4a3tzdMTExgaWmJoKCgSjsmIiIiqt10TpAWLFiAqVOnYv/+/bhz5w6ys7PVXrrYsmULIiIiEBUVhRMnTsDDwwNKpRIZGRll1j969CiGDBmCsLAwJCcnIygoCEFBQThz5oxUZ+HChVi2bBlWrVqFxMREmJqaQqlUIjc3V6rz/fff45133kFoaChOnTqFI0eO4K233tK1KYiIiKiOkgkhhC4rGBg8zqk0xx49zSBtb29vdOnSBStWrAAAFBcXw9HREeHh4Zg2bVqp+sHBwcjJycHOnTulMh8fH3h6emLVqlUQQsDBwQGTJk3C5MmTAQBZWVmwtbXF2rVrMXjwYBQWFsLZ2RmzZ89GWFiYLoeuJjs7GwqFAllZWTA3N3/q7dCzWRp36ZnW90lbXUmRVJ/fmo3SdwhVbuJLrfQdAhHVUdr+/dZ5DNK+ffueKbAS+fn5SEpKQmRkpFRmYGCAgIAAJCQklLlOQkICIiIi1MqUSiW2b98O4PFs3iqVCgEBAdJyhUIBb29vJCQkYPDgwThx4gSuX78OAwMDdOzYESqVCp6enli0aBHat29fbrx5eXnIy8uTPuvaW0ZERES1h84Jkr+/f6Xs+Pbt2ygqKir1eBJbW1tcuHChzHVUKlWZ9VUqlbS8pKy8OlevXgUAfPTRR/jkk0/g7OyMJUuWoGfPnrh06RIaNWpU5r6jo6Mxe/ZsHY+SiIiIaiOdxyABwKFDh/D222/Dz88P169fBwCsX78ehw8frtTgqkLJxJYffvghBg4cCC8vL6xZswYymQxbt24td73IyEhkZWVJr2vXrlVXyERERFTNdE6Qvv/+eyiVSpiYmODEiRPSZaesrCzMmzdP6+1YWVnB0NAQ6enpauXp6emws7Mrcx07O7sK65d8raiOvb09gMd345UwNjZG8+bNkZaWVm68xsbGMDc3V3sRERFR3aRzgjR37lysWrUKX375JerXry+Vd+vWDSdOnNB6O0ZGRvDy8kJ8fLxUVlxcjPj4ePj6+pa5jq+vr1p9AIiLi5Pqu7i4wM7OTq1OdnY2EhMTpTpeXl4wNjbGxYsXpToFBQX4888/4eTkpHX8REREVHfpPAbp4sWL6NGjR6lyhUKBzMxMnbYVERGBkJAQdO7cGV27dkVMTAxycnIQGhoKABg2bBiaNGmC6OhoAMD48ePh7++PJUuWIDAwEJs3b8bx48exevXjO5FkMhkmTJiAuXPnwtXVFS4uLpg5cyYcHBykeY7Mzc3x3nvvISoqCo6OjnBycsKiRYsAAG+++aauzUFERER1kM4Jkp2dHVJSUuDs7KxWfvjwYTRv3lynbQUHB+PWrVuYNWuWdDdZbGysNMg6LS1NmlYAAPz8/LBx40bMmDED06dPh6urK7Zv365299nUqVORk5ODUaNGITMzE927d0dsbCzkcrlUZ9GiRahXrx7eeecdPHr0CN7e3ti7dy8sLS11bQ4iIiKqg3SeByk6OhrffPMN/u///g8vvfQSdu3ahb/++gsTJ07EzJkzER4eXlWx1iicB6lm4DxIdRPnQSKiqlJl8yBNmzYNxcXF6N27Nx4+fIgePXrA2NgYkydPfm6SIyIiIqrbdE6QZDIZPvzwQ0yZMgUpKSl48OAB2rZtCzMzs6qIj4iIiKja6ZwgAY8fK5KdnQ1bW1u12+WJiIiI6gKdbvNXqVQYNmwYLC0tYWtrCxsbG1haWuLdd98tNfcQERERUW2ldQ9SdnY2/Pz88ODBA4SGhsLNzQ1CCJw7dw6bNm3C4cOHceLECV5qIyIiolpP6wTp008/haGhIc6ePQtra2u1ZTNmzEC3bt2wbNkyTJ8+vdKDJCIiIqpOWl9i+/nnnzF9+vRSyREA2NjYIDIyEj/99FOlBkdERESkD1onSJcuXYKfn1+5y/38/NQe30FERERUW2mdIGVnZ8PCwqLc5RYWFsjOzq6MmIiIiIj0SusESQih9tgPTTKZDDpOyk1ERERUI2k9SFsIgVatWkEmk5W7nIiIiKgu0DpBWrNmTVXGQURERFRjaJ0ghYSEVGUcRERERDWGTjNpExERET0PmCARERERaWCCRERERKSBCRIRERGRBp0TpH379lVFHEREREQ1hs4JUp8+fdCiRQvMnTsX165dq4qYiIiIiPRK5wTp+vXrGDt2LL777js0b94cSqUS3377LfLz86siPiIiIqJqp3OCZGVlhYkTJ+LkyZNITExEq1at8MEHH8DBwQHjxo3DqVOnqiJOIiIiomrzTIO0O3XqhMjISIwdOxYPHjzA//3f/8HLywsvvPACzp49W1kxEhEREVWrp0qQCgoK8N1336Fv375wcnLCL7/8ghUrViA9PR0pKSlwcnLCm2++WdmxEhEREVULrR81UiI8PBybNm2CEALvvPMOFi5ciPbt20vLTU1NsXjxYjg4OFRqoERERETVRecE6dy5c1i+fDkGDBgAY2PjMutYWVlxOgAiIiKqtXS+xBYVFYU333yzVHJUWFiIgwcPAgDq1asHf3//yomQiIiIqJrp3IP04osv4ubNm7CxsVErz8rKwosvvoiioqJKC46I6gaftNW6rbCvcdUEoosXI/UdARHpkc49SEIIyGSyUuV37tyBqalppQRFREREpE9a9yANGDAAACCTyTB8+HC1S2xFRUU4ffo0/Pz8Kj9CIiIiomqmdYKkUCgAPO5BatiwIUxMTKRlRkZG8PHxwciRIys/QiIiIqJqpnWCtGbNGgCAs7MzJk+ezMtpREREVGfpPEg7KiqqKuIgIiIiqjG0SpA6deqE+Ph4WFpaomPHjmUO0i5x4sSJSguOiIiISB+0SpD69+8vDcoOCgqqyniIiIiI9E6rBOnfl9V4iY2IiIjquqd6WC0RERFRXaZVD5KlpWWF447+7e7du88UEBFVTOdZqYmISGdaJUgxMTFVHAYRERFRzaFVghQSElLVcRARERHVGFolSNnZ2TA3N5feV6SkHhEREVFtpfUYpJs3b8LGxgYWFhZljkcqeYhtUVFRpQdJREREVJ20SpD27t2LRo0aAQD27dtXpQERERER6ZtWCZK/v3+Z74mIiIjqIp2fxQYA9+7dw//+9z+cP38eANC2bVuEhoZKvUxEREREtZnOE0UePHgQzs7OWLZsGe7du4d79+5h2bJlcHFxwcGDB6siRiIiIqJqpXMP0pgxYxAcHIzPP/8choaGAICioiJ88MEHGDNmDP74449KD5KIiIioOuncg5SSkoJJkyZJyREAGBoaIiIiAikpKZUaHBEREZE+6JwgderUSRp79G/nz5+Hh4dHpQRFREREpE9aXWI7ffq09H7cuHEYP348UlJS4OPjAwD47bffsHLlSsyfP79qoiQiIiKqRlolSJ6enpDJZBBCSGVTp04tVe+tt95CcHBw5UVHREREpAdaJUipqalVHQcRERFRjaFVguTk5FTVcRARERHVGE81USQAnDt3DmlpacjPz1crf+211545KCIiIiJ90jlBunr1Kl5//XX88ccfauOSSh5gy4fVEhERUW2n823+48ePh4uLCzIyMtCgQQOcPXsWBw8eROfOnbF///4qCJGIiIioeuncg5SQkIC9e/fCysoKBgYGMDAwQPfu3REdHY1x48YhOTm5KuIkIiIiqjY69yAVFRWhYcOGAAArKyvcuHEDwOOB3BcvXqzc6IiIiIj0QOcepPbt2+PUqVNwcXGBt7c3Fi5cCCMjI6xevRrNmzevihiJiIiIqpXOCdKMGTOQk5MDAJgzZw5effVVvPDCC2jcuDG2bNlS6QESERERVTedEySlUim9b9myJS5cuIC7d+/C0tJSupONiIiIqDZ76nmQAODatWsAAEdHx0oJhoiIiKgm0HmQdmFhIWbOnAmFQgFnZ2c4OztDoVBgxowZKCgoeKogVq5cCWdnZ8jlcnh7e+PYsWMV1t+6dSvc3Nwgl8vh7u6OXbt2qS0XQmDWrFmwt7eHiYkJAgICcPny5TK3lZeXJz1r7uTJk08VPxEREdUtOidI4eHhWL16NRYuXIjk5GQkJydj4cKF+N///odx48bpHMCWLVsQERGBqKgonDhxAh4eHlAqlcjIyCiz/tGjRzFkyBCEhYUhOTkZQUFBCAoKwpkzZ6Q6CxcuxLJly7Bq1SokJibC1NQUSqUSubm5pbY3depUODg46Bw3ERER1V0yUTIVtpYUCgU2b96MV155Ra18165dGDJkCLKysnQKwNvbG126dMGKFSsAAMXFxXB0dER4eDimTZtWqn5wcDBycnKwc+dOqczHxweenp5YtWoVhBBwcHDApEmTMHnyZABAVlYWbG1tsXbtWgwePFhab/fu3YiIiMD333+Pdu3aITk5GZ6enlrFnZ2dDYVCgaysLJibm+t0zFR5lsZdeqb1fdJWV1IkVJl8mzfWdwjAi5H6joCIqoC2f7917kEyNjaGs7NzqXIXFxcYGRnptK38/HwkJSUhICDgn4AMDBAQEICEhIQy10lISFCrDzweOF5SPzU1FSqVSq2OQqGAt7e32jbT09MxcuRIrF+/Hg0aNHhirHl5ecjOzlZ7ERERUd2kc4I0duxY/Pe//0VeXp5UlpeXh48//hhjx47VaVu3b99GUVERbG1t1cptbW2hUqnKXEelUlVYv+RrRXWEEBg+fDjee+89dO7cWatYo6OjoVAopBcHphMREdVdWt3FNmDAALXPv/76K5o2bQoPDw8AwKlTp5Cfn4/evXtXfoRVYPny5bh//z4iI7XvQo+MjERERIT0OTs7m0kSERFRHaVVgqRQKNQ+Dxw4UO3z0yYKVlZWMDQ0RHp6ulp5eno67OzsylzHzs6uwvolX9PT02Fvb69Wp2R80d69e5GQkABjY2O17XTu3BlDhw7FunXrSu3X2Ni4VH0iIiKqm7RKkNasWVMlOzcyMoKXlxfi4+MRFBQE4PEg7fj4+HIv1/n6+iI+Ph4TJkyQyuLi4uDr6wvg8VgoOzs7xMfHSwlRdnY2EhMT8f777wMAli1bhrlz50rr37hxA0qlElu2bIG3t3flHygRERHVKk89UeStW7ekh9O2bt0a1tbWT7WdiIgIhISEoHPnzujatStiYmKQk5OD0NBQAMCwYcPQpEkTREdHAwDGjx8Pf39/LFmyBIGBgdi8eTOOHz+O1asf340kk8kwYcIEzJ07F66urnBxccHMmTPh4OAgJWHNmjVTi8HMzAwA0KJFCzRt2vSpjoOIiIjqDp0TpJycHISHh+Prr79GcXExAMDQ0BDDhg3D8uXLtboj7N+Cg4Nx69YtzJo1CyqVCp6enoiNjZUGWaelpcHA4J+x5H5+fti4cSNmzJiB6dOnw9XVFdu3b0f79u2lOlOnTkVOTg5GjRqFzMxMdO/eHbGxsZDL5boeLhERET2HdJ4HafTo0fj111+xYsUKdOvWDQBw+PBhjBs3Di+99BI+//zzKgm0puE8SDUD50GqmzgPEhFVFW3/fuvcg/T999/ju+++Q8+ePaWyvn37wsTEBIMGDXpuEiQiIiKqu3SeB+nhw4el5hgCABsbGzx8+LBSgiIiIiLSJ50TJF9fX0RFRak91+zRo0eYPXu2dCcZERERUW2m8yW2mJgY9OnTp9REkXK5HL/88kulB0hERERU3XROkNzd3XH58mVs2LABFy5cAAAMGTIEQ4cOhYmJSaUHSERERFTddEqQCgoK4Obmhp07d2LkyJFVFRMRkf7ti9Z3BLrjnXdElUanMUj169dXG3tEREREVBfpPEh7zJgxWLBgAQoLC6siHiIiIiK903kM0u+//474+Hjs2bMH7u7uMDU1VVv+ww8/VFpwRERERPqgc4JkYWGBgQMHVkUsRERERDWCzgnSmjVrqiIOIiIiohpD6zFIxcXFWLBgAbp164YuXbpg2rRpePToUVXGRkRERKQXWidIH3/8MaZPnw4zMzM0adIEn376KcaMGVOVsRERERHphdYJ0tdff43PPvsMv/zyC7Zv346ffvoJGzZsQHFxcVXGR0RERFTttE6Q0tLS0LdvX+lzQEAAZDIZbty4USWBEREREemL1glSYWEh5HK5Wln9+vVRUFBQ6UERERER6ZPWd7EJITB8+HAYGxtLZbm5uXjvvffU5kLiPEhERERU22mdIIWEhJQqe/vttys1GCIiIqKaQOsEifMfERER0fNC52exEREREdV1TJCIiIiINDBBIiIiItLABImIiIhIAxMkIiIiIg1a38VGRFRdEq7e0ct+fZs31st+iajmYYJElWNftF5265Omnz+kRERUt/ESGxEREZEGJkhEREREGpggEREREWlggkRERESkgYO0qdLo684jIiKiysYeJCIiIiINTJCIiIiINDBBIiIiItLABImIiIhIAxMkIiIiIg1MkIiIiIg0MEEiIiIi0sAEiYiIiEgDEyQiIiIiDUyQiIiIiDQwQSIiIiLSwASJiIiISAMTJCIiIiINTJCIiIiINNTTdwBERFRJ9kXrOwLdvRip7wiIysQeJCIiIiINTJCIiIiINDBBIiIiItLABImIiIhIAxMkIiIiIg1MkIiIiIg0MEEiIiIi0sAEiYiIiEgDEyQiIiIiDUyQiIiIiDQwQSIiIiLSwASJiIiISAMTJCIiIiINTJCIiIiINDBBIiIiItJQIxKklStXwtnZGXK5HN7e3jh27FiF9bdu3Qo3NzfI5XK4u7tj165dasuFEJg1axbs7e1hYmKCgIAAXL58WVr+559/IiwsDC4uLjAxMUGLFi0QFRWF/Pz8Kjk+IiIiql30niBt2bIFERERiIqKwokTJ+Dh4QGlUomMjIwy6x89ehRDhgxBWFgYkpOTERQUhKCgIJw5c0aqs3DhQixbtgyrVq1CYmIiTE1NoVQqkZubCwC4cOECiouL8cUXX+Ds2bNYunQpVq1ahenTp1fLMRMREVHNJhNCCH0G4O3tjS5dumDFihUAgOLiYjg6OiI8PBzTpk0rVT84OBg5OTnYuXOnVObj4wNPT0+sWrUKQgg4ODhg0qRJmDx5MgAgKysLtra2WLt2LQYPHlxmHIsWLcLnn3+Oq1evahV3dnY2FAoFsrKyYG5uruth1z37opFw9Y6+oyB6Jr7NG+s7hOfPi5H6joCeM9r+/dZrD1J+fj6SkpIQEBAglRkYGCAgIAAJCQllrpOQkKBWHwCUSqVUPzU1FSqVSq2OQqGAt7d3udsEHidRjRo1Knd5Xl4esrOz1V5ERERUN+k1Qbp9+zaKiopga2urVm5rawuVSlXmOiqVqsL6JV912WZKSgqWL1+O0aNHlxtrdHQ0FAqF9HJ0dKz44IiIiKjW0vsYJH27fv06+vTpgzfffBMjR44st15kZCSysrKk17Vr16oxSiIiIqpOek2QrKysYGhoiPT0dLXy9PR02NnZlbmOnZ1dhfVLvmqzzRs3buDFF1+En58fVq9eXWGsxsbGMDc3V3sRERFR3aTXBMnIyAheXl6Ij4+XyoqLixEfHw9fX98y1/H19VWrDwBxcXFSfRcXF9jZ2anVyc7ORmJioto2r1+/jp49e8LLywtr1qyBgcFz35lGRERE/189fQcQERGBkJAQdO7cGV27dkVMTAxycnIQGhoKABg2bBiaNGmC6OhoAMD48ePh7++PJUuWIDAwEJs3b8bx48elHiCZTIYJEyZg7ty5cHV1hYuLC2bOnAkHBwcEBQUB+Cc5cnJywuLFi3Hr1i0pnvJ6roiIiOj5ofcEKTg4GLdu3cKsWbOgUqng6emJ2NhYaZB1WlqaWu+On58fNm7ciBkzZmD69OlwdXXF9u3b0b59e6nO1KlTkZOTg1GjRiEzMxPdu3dHbGws5HI5gMc9TikpKUhJSUHTpk3V4tHzrAdERERUA+h9HqTaivMgaeA8SFQHcB4kPeA8SFTNasU8SEREREQ1ERMkIiIiIg1MkIiIiIg0MEEiIiIi0sAEiYiIiEgDEyQiIiIiDUyQiIiIiDQwQSIiIiLSwASJiIiISAMTJCIiIiINTJCIiIiINDBBIiIiItLABImIiIhIAxMkIiIiIg1MkIiIiIg0MEEiIiIi0sAEiYiIiEgDEyQiIiIiDUyQiIiIiDQwQSIiIiLSwASJiIiISAMTJCIiIiINTJCIiIiINDBBIiIiItJQT98BEBHVFAlX7+g7hGrj27yxvkN4bF+0viPQ3YuR+o6AqgF7kIiIiIg0MEEiIiIi0sAEiYiIiEgDEyQiIiIiDUyQiIiIiDQwQSIiIiLSwASJiIiISAMTJCIiIiINTJCIiIiINHAmbSKi55A+Zg2vMbN3P484Y7nO2INEREREpIEJEhEREZEGJkhEREREGpggEREREWlggkRERESkgQkSERERkQYmSEREREQamCARERERaWCCRERERKSBM2nXRLVxxlMioucFf0c/F9iDRERERKSBCRIRERGRBiZIRERERBqYIBERERFp4CDtOijh6h19h0BERFSrsQeJiIiISAMTJCIiIiINTJCIiIiINDBBIiIiItLABImIiIhIAxMkIiIiIg1MkIiIiIg0MEEiIiIi0sAEiYiIiEgDEyQiIiIiDTUiQVq5ciWcnZ0hl8vh7e2NY8eOVVh/69atcHNzg1wuh7u7O3bt2qW2XAiBWbNmwd7eHiYmJggICMDly5fV6ty9exdDhw6Fubk5LCwsEBYWhgcPHlT6sREREVHto/cEacuWLYiIiEBUVBROnDgBDw8PKJVKZGRklFn/6NGjGDJkCMLCwpCcnIygoCAEBQXhzJkzUp2FCxdi2bJlWLVqFRITE2FqagqlUonc3FypztChQ3H27FnExcVh586dOHjwIEaNGlXlx0tEREQ1n0wIIfQZgLe3N7p06YIVK1YAAIqLi+Ho6Ijw8HBMmzatVP3g4GDk5ORg586dUpmPjw88PT2xatUqCCHg4OCASZMmYfLkyQCArKws2NraYu3atRg8eDDOnz+Ptm3b4vfff0fnzp0BALGxsejbty/+/vtvODg4PDHu7OxsKBQKZGVlwdzcvDKa4h/7op9pdT6slohqIt/mjfUdAtUmL0ZWyWa1/fut1x6k/Px8JCUlISAgQCozMDBAQEAAEhISylwnISFBrT4AKJVKqX5qaipUKpVaHYVCAW9vb6lOQkICLCwspOQIAAICAmBgYIDExMRKOz4iIiKqnerpc+e3b99GUVERbG1t1cptbW1x4cKFMtdRqVRl1lepVNLykrKK6tjY2Kgtr1evHho1aiTV0ZSXl4e8vDzpc1ZWFoDHmWily8l9cp2KVn+U9+RKRETVLPsZf7fRc6Yq/r7in7/bT7qAptcEqTaJjo7G7NmzS5U7OjrqIRoiIqK6bk6Vbv3+/ftQKBTlLtdrgmRlZQVDQ0Okp6erlaenp8POzq7Mdezs7CqsX/I1PT0d9vb2anU8PT2lOpqDwAsLC3H37t1y9xsZGYmIiAjpc3FxMe7evYvGjRtDJpOVuU52djYcHR1x7dq1yh+nVIuwHR5jOzzGdvgH2+IxtsNjbIfHqrodhBC4f//+E8cb6zVBMjIygpeXF+Lj4xEUFATgceIRHx+PsWPHlrmOr68v4uPjMWHCBKksLi4Ovr6+AAAXFxfY2dkhPj5eSoiys7ORmJiI999/X9pGZmYmkpKS4OXlBQDYu3cviouL4e3tXeZ+jY2NYWxsrFZmYWGh1XGam5s/1yd7CbbDY2yHx9gO/2BbPMZ2eIzt8FhVtkNFPUcl9H6JLSIiAiEhIejcuTO6du2KmJgY5OTkIDQ0FAAwbNgwNGnSBNHRj+/sGj9+PPz9/bFkyRIEBgZi8+bNOH78OFavXg0AkMlkmDBhAubOnQtXV1e4uLhg5syZcHBwkJKwNm3aoE+fPhg5ciRWrVqFgoICjB07FoMHD9bqDjYiIiKq2/SeIAUHB+PWrVuYNWsWVCoVPD09ERsbKw2yTktLg4HBPzfb+fn5YePGjZgxYwamT58OV1dXbN++He3bt5fqTJ06FTk5ORg1ahQyMzPRvXt3xMbGQi6XS3U2bNiAsWPHonfv3jAwMMDAgQOxbNmy6jtwIiIiqrkEVZnc3FwRFRUlcnNz9R2KXrEdHmM7PMZ2+Afb4jG2w2Nsh8dqSjvofaJIIiIioppG748aISIiIqppmCARERERaWCCRERERKSBCRIRERGRBiZIVWjlypVwdnaGXC6Ht7c3jh07pu+QqtRHH30EmUym9nJzc5OW5+bmYsyYMWjcuDHMzMwwcODAUrOi10YHDx5Ev3794ODgAJlMhu3bt6stF0Jg1qxZsLe3h4mJCQICAnD58mW1Onfv3sXQoUNhbm4OCwsLhIWF4cGDB9V4FM/uSe0wfPjwUudHnz591OrU9naIjo5Gly5d0LBhQ9jY2CAoKAgXL15Uq6PNz0FaWhoCAwPRoEED2NjYYMqUKSgsLKzOQ3lm2rRFz549S50T7733nlqd2t4Wn3/+OTp06CBNeujr64vdu3dLy5+X8+FJ7VATzwUmSFVky5YtiIiIQFRUFE6cOAEPDw8olcpSjzipa9q1a4ebN29Kr8OHD0vLJk6ciJ9++glbt27FgQMHcOPGDQwYMECP0VaOnJwceHh4YOXKlWUuX7hwIZYtW4ZVq1YhMTERpqamUCqVyM3958GdQ4cOxdmzZxEXF4edO3fi4MGDGDVqVHUdQqV4UjsAQJ8+fdTOj02bNqktr+3tcODAAYwZMwa//fYb4uLiUFBQgJdffhk5OTlSnSf9HBQVFSEwMBD5+fk4evQo1q1bh7Vr12LWrFn6OKSnpk1bAMDIkSPVzomFCxdKy+pCWzRt2hTz589HUlISjh8/jl69eqF///44e/YsgOfnfHhSOwA18FzQ6yQDdVjXrl3FmDFjpM9FRUXCwcFBREdH6zGqqhUVFSU8PDzKXJaZmSnq168vtm7dKpWdP39eABAJCQnVFGHVAyC2bdsmfS4uLhZ2dnZi0aJFUllmZqYwNjYWmzZtEkIIce7cOQFA/P7771Kd3bt3C5lMJq5fv15tsVcmzXYQQoiQkBDRv3//ctepi+2QkZEhAIgDBw4IIbT7Odi1a5cwMDAQKpVKqvP5558Lc3NzkZeXV70HUIk020IIIfz9/cX48ePLXaeutoWlpaX46quvnuvzQYh/2kGImnkusAepCuTn5yMpKQkBAQFSmYGBAQICApCQkKDHyKre5cuX4eDggObNm2Po0KFIS0sDACQlJaGgoECtTdzc3NCsWbM63SapqalQqVRqx61QKODt7S0dd0JCAiwsLNC5c2epTkBAAAwMDJCYmFjtMVel/fv3w8bGBq1bt8b777+PO3fuSMvqYjtkZWUBABo1agRAu5+DhIQEuLu7S08TAAClUons7Gy1/7ZrG822KLFhwwZYWVmhffv2iIyMxMOHD6Vlda0tioqKsHnzZuTk5MDX1/e5PR8026FETTsX9P6okbro9u3bKCoqUvtGAoCtrS0uXLigp6iqnre3N9auXYvWrVvj5s2bmD17Nl544QWcOXMGKpUKRkZGpR7wa2trC5VKpZ+Aq0HJsZV1LpQsU6lUsLGxUVter149NGrUqE61TZ8+fTBgwAC4uLjgypUrmD59Ol555RUkJCTA0NCwzrVDcXExJkyYgG7dukmPQtLm50ClUpV5vpQsq43KagsAeOutt+Dk5AQHBwecPn0a//nPf3Dx4kX88MMPAOpOW/zxxx/w9fVFbm4uzMzMsG3bNrRt2xYnT558rs6H8toBqJnnAhMkqjSvvPKK9L5Dhw7w9vaGk5MTvv32W5iYmOgxMqoJBg8eLL13d3dHhw4d0KJFC+zfvx+9e/fWY2RVY8yYMThz5ozaOLznVXlt8e/xZe7u7rC3t0fv3r1x5coVtGjRorrDrDKtW7fGyZMnkZWVhe+++w4hISE4cOCAvsOqduW1Q9u2bWvkucBLbFXAysoKhoaGpe5ESE9Ph52dnZ6iqn4WFhZo1aoVUlJSYGdnh/z8fGRmZqrVqettUnJsFZ0LdnZ2pQbvFxYW4u7du3W6bZo3bw4rKyukpKQAqFvtMHbsWOzcuRP79u1D06ZNpXJtfg7s7OzKPF9KltU25bVFWby9vQFA7ZyoC21hZGSEli1bwsvLC9HR0fDw8MCnn3763J0P5bVDWWrCucAEqQoYGRnBy8sL8fHxUllxcTHi4+PVrrfWdQ8ePMCVK1dgb28PLy8v1K9fX61NLl68iLS0tDrdJi4uLrCzs1M77uzsbCQmJkrH7evri8zMTCQlJUl19u7di+LiYumXRF30999/486dO7C3twdQN9pBCIGxY8di27Zt2Lt3L1xcXNSWa/Nz4Ovriz/++EMtWYyLi4O5ubl0OaI2eFJblOXkyZMAoHZO1IW20FRcXIy8vLzn6nwoS0k7lKVGnAtVMvSbxObNm4WxsbFYu3atOHfunBg1apSwsLBQG4Ff10yaNEns379fpKamiiNHjoiAgABhZWUlMjIyhBBCvPfee6JZs2Zi79694vjx48LX11f4+vrqOepnd//+fZGcnCySk5MFAPHJJ5+I5ORk8ddffwkhhJg/f76wsLAQO3bsEKdPnxb9+/cXLi4u4tGjR9I2+vTpIzp27CgSExPF4cOHhaurqxgyZIi+DumpVNQO9+/fF5MnTxYJCQkiNTVV/Prrr6JTp07C1dVV7Yndtb0d3n//faFQKMT+/fvFzZs3pdfDhw+lOk/6OSgsLBTt27cXL7/8sjh58qSIjY0V1tbWIjIyUh+H9NSe1BYpKSlizpw54vjx4yI1NVXs2LFDNG/eXPTo0UPaRl1oi2nTpokDBw6I1NRUcfr0aTFt2jQhk8nEnj17hBDPz/lQUTvU1HOBCVIVWr58uWjWrJkwMjISXbt2Fb/99pu+Q6pSwcHBwt7eXhgZGYkmTZqI4OBgkZKSIi1/9OiR+OCDD4SlpaVo0KCBeP3118XNmzf1GHHl2LdvnwBQ6hUSEiKEeHyr/8yZM4Wtra0wNjYWvXv3FhcvXlTbxp07d8SQIUOEmZmZMDc3F6GhoeL+/ft6OJqnV1E7PHz4ULz88svC2tpa1K9fXzg5OYmRI0eW+oehtrdDWccPQKxZs0aqo83PwZ9//ileeeUVYWJiIqysrMSkSZNEQUFBNR/Ns3lSW6SlpYkePXqIRo0aCWNjY9GyZUsxZcoUkZWVpbad2t4W7777rnBychJGRkbC2tpa9O7dW0qOhHh+zoeK2qGmngsyIYSomr4pIiIiotqJY5CIiIiINDBBIiIiItLABImIiIhIAxMkIiIiIg1MkIiIiIg0MEEiIiIi0sAEiYiIiEgDEyQiqjHWrl1b6snm2po5c6baAy/17c8//4RMJpMemVATXLhwAT4+PpDL5fD09KzUbev7eG/fvg0bGxv8/fffetk/1T1MkIiqwK1bt/D++++jWbNmMDY2hp2dHZRKJY4cOaLv0J4pCalMzs7OiImJqZRtqVQqfPrpp/jwww+lsuHDh0Mmk2H+/Plqdbdv3w6ZTFYp+61toqKiYGpqiosXL6o9/6uETCar8PXRRx9Vf9BasrKywrBhwxAVFaXvUKiOqKfvAIjqooEDByI/Px/r1q1D8+bNkZ6ejvj4eNy5c0ffodVJX331Ffz8/ODk5KRWLpfLsWDBAowePRqWlpZ6iq5y5efnw8jI6KnWvXLlCgIDA0u1U4mbN29K77ds2YJZs2bh4sWLUpmZmdlT7be6hIaGwsvLC4sWLUKjRo30HQ7VcuxBIqpkmZmZOHToEBYsWIAXX3wRTk5O6Nq1KyIjI/Haa69J9dLS0tC/f3+YmZnB3NwcgwYNQnp6urT8o48+gqenJ9avXw9nZ2coFAoMHjwY9+/fl+rcv38fQ4cOhampKezt7bF06VL07NkTEyZMeKb4R4wYAWtra5ibm6NXr144depUpcbVs2dP/PXXX5g4caLUO/Fvv/zyC9q0aQMzMzP06dNH7Q93WTZv3ox+/fqVKg8ICICdnR2io6PLXbfkeP4tJiYGzs7O0ufhw4cjKCgI8+bNg62tLSwsLDBnzhwUFhZiypQpaNSoEZo2bYo1a9aU2v6FCxfg5+cHuVyO9u3b48CBA2rLz5w5g1deeQVmZmawtbXFO++8g9u3b0vLe/bsibFjx2LChAmwsrKCUqks8ziKi4sxZ84cNG3aFMbGxvD09ERsbKy0XCaTISkpCXPmzCm3N8jOzk56KRQKyGQy6bONjQ0++eSTcrevqaioCO+++y7c3NyQlpYGANixYwc6deoEuVyO5s2bY/bs2SgsLFSL8auvvsLrr7+OBg0awNXVFT/++KO0/N69exg6dCisra1hYmICV1dXtTZv164dHBwcsG3btnLjItIWEySiSmZmZgYzMzNs374deXl5ZdYpLi5G//79cffuXRw4cABxcXG4evUqgoOD1epduXIF27dvx86dO7Fz504cOHBA7ZJRREQEjhw5gh9//BFxcXE4dOgQTpw48Uzxv/nmm8jIyMDu3buRlJSETp06oXfv3rh7926lxfXDDz+gadOmmDNnDm7evKmWAD18+BCLFy/G+vXrcfDgQaSlpWHy5Mnlxnv37l2cO3cOnTt3LrXM0NAQ8+bNw/Lly595bMrevXtx48YNHDx4EJ988gmioqLw6quvwtLSEomJiXjvvfcwevToUvuZMmUKJk2ahOTkZPj6+qJfv35ST2JmZiZ69eqFjh074vjx44iNjUV6ejoGDRqkto1169bByMgIR44cwapVq8qM79NPP8WSJUuwePFinD59GkqlEq+99houX74M4HHvULt27TBp0iTcvHmzwjZ9mu3/W15eHt58802cPHkShw4dQrNmzXDo0CEMGzYM48ePx7lz5/DFF19g7dq1+Pjjj9XWnT17NgYNGoTTp0+jb9++GDp0qHTuzZw5E+fOncPu3btx/vx5fP7557CyslJbv2vXrjh06JBOx0ZUpip7DC7Rc+y7774TlpaWQi6XCz8/PxEZGSlOnTolLd+zZ48wNDQUaWlpUtnZs2cFAHHs2DEhhBBRUVGiQYMGIjs7W6ozZcoU4e3tLYQQIjs7W9SvX19s3bpVWp6ZmSkaNGggxo8fX25sa9asEQqFosxlhw4dEubm5iI3N1etvEWLFuKLL76o1LicnJzE0qVLS8UGQKSkpEhlK1euFLa2tuUeT3JysgCg1pZCCBESEiL69+8vhBDCx8dHvPvuu0IIIbZt2yb+/asvKipKeHh4qK27dOlS4eTkpLYtJycnUVRUJJW1bt1avPDCC9LnwsJCYWpqKjZt2iSEECI1NVUAEPPnz5fqFBQUiKZNm4oFCxYIIYT473//K15++WW1fV+7dk0AEBcvXhRCCOHv7y86duxY7vGXcHBwEB9//LFaWZcuXcQHH3wgffbw8BBRUVFP3JYQpc+TJ22/5HgPHTokevfuLbp37y4yMzOlur179xbz5s1TW3/9+vXC3t5e+gxAzJgxQ/r84MEDAUDs3r1bCCFEv379RGhoaIVxT5w4UfTs2VOrYySqCHuQiKrAwIEDcePGDfz444/o06cP9u/fj06dOmHt2rUAgPPnz8PR0RGOjo7SOm3btoWFhQXOnz8vlTk7O6Nhw4bSZ3t7e2RkZAAArl69ioKCAnTt2lVarlAo0Lp166eO+9SpU3jw4AEaN24s9YSZmZkhNTUVV65cqZa4GjRogBYtWpS57bI8evQIwOPxRuVZsGAB1q1bp9a2umrXrh0MDP75lWlrawt3d3fps6GhIRo3blwqVl9fX+l9vXr10LlzZymOU6dOYd++fWpt7ebmBgBq7e3l5VVhbNnZ2bhx4wa6deumVt6tW7dnOuan2f6QIUOQk5ODPXv2QKFQSOWnTp3CnDlz1I515MiRuHnzJh4+fCjV69Chg/Te1NQU5ubmUpu+//772Lx5Mzw9PTF16lQcPXq0VKwmJiZq2yN6WhykTVRF5HI5XnrpJbz00kuYOXMmRowYgaioKAwfPlzrbdSvX1/ts0wmQ3FxcSVH+o8HDx7A3t4e+/fvL7Xs33e+VWVcZW1bCFFu/ZJLLPfu3YO1tXWZdXr06AGlUonIyMhS7W9gYFBq+wUFBVrF9azt8ODBA/Tr1w8LFiwotcze3l56b2pqqvU29a1v37745ptvkJCQgF69eknlDx48wOzZszFgwIBS6/w7ua2oTV955RX89ddf2LVrF+Li4tC7d2+MGTMGixcvlurfvXu33POASBfsQSKqJm3btkVOTg4AoE2bNrh27RquXbsmLT937hwyMzPRtm1brbbXvHlz1K9fH7///rtUlpWVhUuXLj11jJ06dYJKpUK9evXQsmVLtZfmWI9njcvIyAhFRUVPHWuJFi1awNzcHOfOnauw3vz58/HTTz8hISFBrdza2hoqlUotSarMuXx+++036X1hYSGSkpLQpk0bAI/b++zZs3B2di7V3rokRebm5nBwcCg1jcSRI0e0Pp8qa/vvv/8+5s+fj9dee01tQHqnTp1w8eLFUsfZsmVLtZ65J7G2tkZISAi++eYbxMTEYPXq1WrLz5w5g44dOz7FURKpYw8SUSW7c+cO3nzzTbz77rvo0KEDGjZsiOPHj2PhwoXo378/gMd3V7m7u2Po0KGIiYlBYWEhPvjgA/j7+5c52LgsDRs2REhIiHQXlY2NDaKiomBgYPDEeX6KiopKJQHGxsYICAiAr68vgoKCsHDhQrRq1Qo3btzAzz//jNdff12r2LSNy9nZGQcPHsTgwYNhbGysdQKmycDAAAEBATh8+DCCgoLKrVfS3suWLVMr79mzJ27duoWFCxfijTfeQGxsLHbv3g1zc/OnikfTypUr4erqijZt2mDp0qW4d+8e3n33XQDAmDFj8OWXX2LIkCGYOnUqGjVqhJSUFGzevBlfffUVDA0Ntd7PlClTEBUVhRYtWsDT0xNr1qzByZMnsWHDhko5Dl22Hx4ejqKiIrz66qvYvXs3unfvjlmzZuHVV19Fs2bN8MYbb8DAwACnTp3CmTNnMHfuXK1imDVrFry8vNCuXTvk5eVh586dUrIJPB7gn5SUhHnz5lXKMdPzjT1IRJXMzMwM3t7eWLp0KXr06IH27dtj5syZGDlyJFasWAHg8WWDHTt2wNLSEj169EBAQACaN2+OLVu26LSvTz75BL6+vnj11VcREBCAbt26oU2bNhWOxwEeX+7o2LGj2qtfv36QyWTYtWsXevTogdDQULRq1QqDBw/GX3/9BVtb20qNa86cOfjzzz/RokWLZ74kMmLECGzevPmJl7fmzJlTqk6bNm3w2WefYeXKlfDw8MCxY8d0vsOrIvPnz8f8+fPh4eGBw4cP48cff5SSwZJemaKiIrz88stwd3fHhAkTYGFhoVOvCgCMGzcOERERmDRpEtzd3REbG4sff/wRrq6ulXIcum5/woQJmD17Nvr27YujR49CqVRi586d2LNnD7p06QIfHx8sXbq03DmZymJkZITIyEh06NABPXr0gKGhITZv3iwt37FjB5o1a4YXXnjhmY+XSCYqurhPRLVKTk4OmjRpgiVLliAsLEzf4UiqOi4hBLy9vTFx4kQMGTKk0rdPtYOPjw/GjRuHt956S9+hUB3AS2xEtVhycjIuXLiArl27IisrC3PmzAEA6VLe8xKXTCbD6tWr8ccff1TJ9qnmu337NgYMGMAEmSoNe5CIarHk5GSMGDECFy9ehJGREby8vPDJJ5+o3X7OuIiIdMcEiYiIiEgDB2kTERERaWCCRERERKSBCRIRERGRBiZIRERERBqYIBERERFpYIJEREREpIEJEhEREZEGJkhEREREGpggEREREWn4f1cXWpFvzE0VAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create a new dataframe with song lengths\n",
    "song_lengths = lyrics_data.groupby('artist')['lyrics_clean'].apply(lambda x: x.str.len()).reset_index()\n",
    "\n",
    "# Plot the histogram\n",
    "song_lengths.groupby('artist')['lyrics_clean'].plot(kind=\"hist\", density=True, alpha=0.5, legend=True)\n",
    "# Add labels and title to the plot\n",
    "plt.xlabel('Song Length (Number of Tokens)')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Histogram of Song Lengths by Artist')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e331e8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>level_1</th>\n",
       "      <th>lyrics_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td>104</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>105</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>106</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher</td>\n",
       "      <td>107</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher</td>\n",
       "      <td>108</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>robyn</td>\n",
       "      <td>99</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>robyn</td>\n",
       "      <td>100</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>robyn</td>\n",
       "      <td>101</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>robyn</td>\n",
       "      <td>102</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>robyn</td>\n",
       "      <td>103</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist  level_1  lyrics_clean\n",
       "0     cher      104            80\n",
       "1     cher      105           147\n",
       "2     cher      106           160\n",
       "3     cher      107            82\n",
       "4     cher      108            89\n",
       "..     ...      ...           ...\n",
       "415  robyn       99           114\n",
       "416  robyn      100           173\n",
       "417  robyn      101           188\n",
       "418  robyn      102            67\n",
       "419  robyn      103            96\n",
       "\n",
       "[420 rows x 3 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: The \\s is used to match a single whitespace character (tab, newline), the + is used to match one or more of the preceding element which mean one or more white space characters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fd968379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"include, me, out\", it, is, really, very, sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"electric\", electric..., it's, electric, it's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"beach, 2k20\", (so, you, wanna, go, out?, how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"love, kills\", if, you're, looking, for, love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"time, machine\", hey,, what, did, i, do?, can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>[\"take, it, from, the, boys\", so, scared, i, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>[\"dream, baby\", i, found, the, boy, for, me, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>[\"please, don't, tell, me\", ya, shook, the, ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>[\"i, hope, you, find, it\", these, clouds, aren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>[\"classified, 1a\", i, know, now, how, much, i,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      tokenized_lyrics\n",
       "0    [\"include, me, out\", it, is, really, very, sim...\n",
       "1    [\"electric\", electric..., it's, electric, it's...\n",
       "2    [\"beach, 2k20\", (so, you, wanna, go, out?, how...\n",
       "3    [\"love, kills\", if, you're, looking, for, love...\n",
       "4    [\"time, machine\", hey,, what, did, i, do?, can...\n",
       "..                                                 ...\n",
       "415  [\"take, it, from, the, boys\", so, scared, i, n...\n",
       "416  [\"dream, baby\", i, found, the, boy, for, me, h...\n",
       "417  [\"please, don't, tell, me\", ya, shook, the, ov...\n",
       "418  [\"i, hope, you, find, it\", these, clouds, aren...\n",
       "419  [\"classified, 1a\", i, know, now, how, much, i,...\n",
       "\n",
       "[420 rows x 1 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_data[['tokenized_lyrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3409065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_data['tokenized_lyrics'] = lyrics_data['lyrics'].apply(tokenize_lyrics)\n",
    "song_lengths_2 = lyrics_data.groupby('artist')['tokenized_lyrics'].apply(lambda x: x.str.len()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2294c440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your lyric length comparison chart here. \n",
    "\n",
    "# Plot the histogram\n",
    "song_lengths_2.groupby('artist')['tokenized_lyrics'].plot(kind=\"hist\", density=True, alpha=0.5, legend=True)\n",
    "# Add labels and title to the plot\n",
    "plt.xlabel('New Song Length (Number of Tokens)')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('New Histogram of Song Lengths by Artist')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
